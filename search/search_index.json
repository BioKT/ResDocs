{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Research Toolbox Here you will find documentation and setup scripts for the tools that I most commonly use. About me System administration: Linux Mac OSX PBS job submission Phylogenetic trees with Beast MD simulations with Gromacs: Installation Simulation setup Molecular visualization using VMD Other stuff","title":"Home"},{"location":"#research-toolbox","text":"Here you will find documentation and setup scripts for the tools that I most commonly use. About me System administration: Linux Mac OSX PBS job submission Phylogenetic trees with Beast MD simulations with Gromacs: Installation Simulation setup Molecular visualization using VMD Other stuff","title":"Research Toolbox"},{"location":"about/","text":"About My name is David De Sancho and currently I am a Ramon y Cajal fellow in the University of the Basque Country and the Donostia International Physics Center . My work focuses on the study of the conformational dynamics of biomolecules with computational methods, primarily molecular simulation. You can find more information about the work we do in my website or check my updated list of publications in Google Scholar . Much of the code I write is hosted in Github . In this website you will find information about a number of common tools that we use in our research.","title":"About"},{"location":"about/#about","text":"My name is David De Sancho and currently I am a Ramon y Cajal fellow in the University of the Basque Country and the Donostia International Physics Center . My work focuses on the study of the conformational dynamics of biomolecules with computational methods, primarily molecular simulation. You can find more information about the work we do in my website or check my updated list of publications in Google Scholar . Much of the code I write is hosted in Github . In this website you will find information about a number of common tools that we use in our research.","title":"About"},{"location":"Beast/installation/","text":"Beast and Beagle installation in a Linux server These instructions are valid for a Linux server running Ubuntu 14.04 LTS, with 2 Intel Xeon processors and 2 Nvidia GPUs. They are are based on those found in the Beast and Beagle sites, plus my own experience. The first thing I had to do was to get all the dependencies right, for which I run the following sudo apt-get install build-essential autoconf automake libtool subversion pkg-config openjdk-6-jdk Once these were all in place, I tried using the NVIDIA OpenCL implementation which was accessible via the apt-get command. However, I was unable to install the Beagle library so that it was recognized. Tests systematically failed. Hence, I resorted to the Intel implementation of OpenCL . After the download, this can be easily installed cd opencl_runtime_16.1.1_x64_ubuntu_6.4.0.25/ sudo bash install.sh Then I went back to the installation of the Beagle library. ./autogen.sh ./configure --prefix=\"/opt/beast/beagle-lib\" --with-opencl=\"/opt/intel/opencl/lib64\" sudo make install In order to check whether I was getting the correct functionality I run the test suite make check Finally all went well. So that allowed running the downloaded Beast executable available here .","title":"Beast"},{"location":"Beast/installation/#beast-and-beagle-installation-in-a-linux-server","text":"These instructions are valid for a Linux server running Ubuntu 14.04 LTS, with 2 Intel Xeon processors and 2 Nvidia GPUs. They are are based on those found in the Beast and Beagle sites, plus my own experience. The first thing I had to do was to get all the dependencies right, for which I run the following sudo apt-get install build-essential autoconf automake libtool subversion pkg-config openjdk-6-jdk Once these were all in place, I tried using the NVIDIA OpenCL implementation which was accessible via the apt-get command. However, I was unable to install the Beagle library so that it was recognized. Tests systematically failed. Hence, I resorted to the Intel implementation of OpenCL . After the download, this can be easily installed cd opencl_runtime_16.1.1_x64_ubuntu_6.4.0.25/ sudo bash install.sh Then I went back to the installation of the Beagle library. ./autogen.sh ./configure --prefix=\"/opt/beast/beagle-lib\" --with-opencl=\"/opt/intel/opencl/lib64\" sudo make install In order to check whether I was getting the correct functionality I run the test suite make check Finally all went well. So that allowed running the downloaded Beast executable available here .","title":"Beast and Beagle installation in a Linux server"},{"location":"Gromacs/benchmarks/","text":"Gromacs benchmarks in our systems Hydrogenase This system consists of a protein with 19054 water molecules plus ions, summing a total of 83764 atoms, in a 9.71602 9.71602 6.87026 nm simulation box. We have run simulations in both the ATLAS and ARINA clusters. Below we show PBS scripts for both supercomputers. ATLAS #!/bin/bash #PBS -q qchem #PBS -l nodes=1:ppn=16 #PBS -l mem=1gb #PBS -l cput=1000:00:00 #PBS -N qchem module load GROMACS/2019.4-foss-2019b cd $PBS_O_WORKDIR export NPROCS=`wc -l < $PBS_NODEFILE` tpr=\"dd_ff03w_tip4p2005_ions0.1_posre.tpr\" np=8 ntomp=2 out=\"ntmpi${np}_ntomp${ntomp}\" gmx mdrun -ntmpi $np -ntomp $ntomp -s $tpr -deffnm $out KALK #!/bin/bash #PBS -l nodes=nd15:ppn=16 #PBS -l walltime=480:00:00 #PBS -joe #PBS -q gpu2 ## Do not change below this line . /home/users/etc/send_funcs.sh HOST=$(hostname) echo Job runing on $HOST echo $PBS_O_WORKDIR > /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER chmod 755 /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER cd $PBS_O_WORKDIR chooseScr cd $scr #echo \"cp -r $PBS_O_WORKDIR/* $scr/.\" cp -r $PBS_O_WORKDIR/*tpr $scr/. #source /software/gromacs/bin/GMXRC.bash #module load GROMACS/2019.4-fosscuda-2018b module load GROMACS/2020-fosscuda-2018b tpr=dd_ff03w_tip4p2005_ions0.1_posre out=$tpr /usr/bin/time -p mpirun -np 1 gmx_mpi mdrun -ntomp 16 -quiet -nb gpu -gpu_id 01 -s $tpr -deffnm $out > $PBS_JOBID.out recover rm -f /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER","title":"Benchmarks"},{"location":"Gromacs/benchmarks/#gromacs-benchmarks-in-our-systems","text":"","title":"Gromacs benchmarks in our systems"},{"location":"Gromacs/benchmarks/#hydrogenase","text":"This system consists of a protein with 19054 water molecules plus ions, summing a total of 83764 atoms, in a 9.71602 9.71602 6.87026 nm simulation box. We have run simulations in both the ATLAS and ARINA clusters. Below we show PBS scripts for both supercomputers.","title":"Hydrogenase"},{"location":"Gromacs/benchmarks/#atlas","text":"#!/bin/bash #PBS -q qchem #PBS -l nodes=1:ppn=16 #PBS -l mem=1gb #PBS -l cput=1000:00:00 #PBS -N qchem module load GROMACS/2019.4-foss-2019b cd $PBS_O_WORKDIR export NPROCS=`wc -l < $PBS_NODEFILE` tpr=\"dd_ff03w_tip4p2005_ions0.1_posre.tpr\" np=8 ntomp=2 out=\"ntmpi${np}_ntomp${ntomp}\" gmx mdrun -ntmpi $np -ntomp $ntomp -s $tpr -deffnm $out","title":"ATLAS"},{"location":"Gromacs/benchmarks/#kalk","text":"#!/bin/bash #PBS -l nodes=nd15:ppn=16 #PBS -l walltime=480:00:00 #PBS -joe #PBS -q gpu2 ## Do not change below this line . /home/users/etc/send_funcs.sh HOST=$(hostname) echo Job runing on $HOST echo $PBS_O_WORKDIR > /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER chmod 755 /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER cd $PBS_O_WORKDIR chooseScr cd $scr #echo \"cp -r $PBS_O_WORKDIR/* $scr/.\" cp -r $PBS_O_WORKDIR/*tpr $scr/. #source /software/gromacs/bin/GMXRC.bash #module load GROMACS/2019.4-fosscuda-2018b module load GROMACS/2020-fosscuda-2018b tpr=dd_ff03w_tip4p2005_ions0.1_posre out=$tpr /usr/bin/time -p mpirun -np 1 gmx_mpi mdrun -ntomp 16 -quiet -nb gpu -gpu_id 01 -s $tpr -deffnm $out > $PBS_JOBID.out recover rm -f /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER","title":"KALK"},{"location":"Gromacs/installation/","text":"Gromacs installation in Mac OS X Building Gromacs 4.* using MAKE One of the prerequisites for the installation are the fftw libraries for doing Fourier transforms. Setting these up correctly seems to be limiting, as in the configure step Gromacs struggled to find the Macports libraries. So first of all, download fftw-3.0.1.tar.gz on your computer. Then you can simply install as ./configure --enable-float --enable-threads make sudo make install Then you can download the source code and proceed to install Gromacs in the usual way ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-float make sudo make install On occasion I have found this install to give Segmentation Faults or not work. In order to make it work it needed a little tweaking, making explicit the compiler and including additional flags, all of which may make a difference CFLAGS=\"-m64 -U_FORTIFY_SOURCE\"; ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-floats --enable-apple-64bit make sudo make install I still need to work out how to make this run in parallel on a Mac. Building Gromacs 5.* using CMAKE (These instructions were borrowed from Phillip W Fowler\u00b4s blog). Prerequisites for the installation are the gcc compilers available at MacPorts. This requires you to first install Xcode. Then the most important thing is to download the source code from the Gromacs website and unpack the software. tar xvf gromacs-5.0.4.tar.gz cd gromacs-5.0.4 mkdir build Then we start with the interestign stuff. In the last few versions Gromacs has made a transition towards using cmake instead of make. Cmake is readily available for Mac OS X, so no problem with this. Then you must run the following in the command line: cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX='/usr/local/gromacs/5.0.4/' make sudo make install This will install the program in /usr/local/gromacs/5.0.4 , and editing your .bashrc file you will be able to choose the version of Gromacs that is running. Adding MPI support on a Mac is trickier. This appears mainly to be because the gcc compilers from MacPorts do not appear to support OpenMPI. Here is a workaround sudo port install openmpi Now we need a compiler that supports OpenMPI sudo port install openmpi-devel-gcc49 Finally, we can follow the steps above but now we need a more complex cmake instruction cmake .. -DGMX_BUILD_OWN_FFTW=ON -DGMX_BUILD_MDRUN_ONLY=on -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/5.0.4 -DGMX_MPI=ON -DCMAKE_C_COMPILER=mpicc-openmpi-devel-gcc49 -DCMAKE_CXX_COMPILER=mpicxx-openmpi-devel-gcc49 -DGMX_SIMD=SSE4.1 make sudo make install-mdrun This is only going to build an MPI version of mdrun , as the other Gromacs programs do not run in parallel. We have to tell cmake what all the new fancy compilers are called and, unfortunately, these don\u2019t support AVX SIMD instructions so we have to fall back to SSE4.1. Experience suggests this doesn\u2019t impact performance as much as you might think. For a previous version of Gromacs (4.6.7) that also happens to be installed using cmake I have found this method not to work well. Instead I did something apparently simpler which did work. CC=mpicc CXX=mpiCC cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/4.6.7 -DGMX_MPI=ON And then one would continue with the make and make install-mdrun steps as before. This allowed for running jobs using the mpirun command as mpirun -np 12 mdrun_mpi -v $OPTIONS Gromacs installation in Linux Ubuntu Server First of all we need to have everything in place for the installation. That is easily done by using Ubuntu's package manager. sudo apt-get install libibnetdisc-dev sudo apt-get install libgsl0ldbl sudo apt-get install openmpi-bin openmpi-common openssh-client openssh-server libopenmpi1.6 libopenmpi-dbg libopenmpi-dev sudo apt-get install cmake Then we dowload the relevant Gromacs version wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-5.1.2.tar.gz We decompress the file and create the build directory for running the installation tar -xvf gromacs-5.1.2.tar.gz cd gromacs-5.1.2/ mkdir build-cmake cd build-cmake/ Finally we use the appropiate flags for building the MPI version of the mdrun program and the non-MPI version of everything else sudo cmake .. -DGMX_GPU=ON -DGMX_BUILD_MDRUN_ONLY=ON -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install sudo cmake .. -DGMX_GPU=ON -DGMX_MPI=OFF -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install","title":"Installation"},{"location":"Gromacs/installation/#gromacs-installation-in-mac-os-x","text":"","title":"Gromacs installation in Mac OS X"},{"location":"Gromacs/installation/#building-gromacs-4-using-make","text":"One of the prerequisites for the installation are the fftw libraries for doing Fourier transforms. Setting these up correctly seems to be limiting, as in the configure step Gromacs struggled to find the Macports libraries. So first of all, download fftw-3.0.1.tar.gz on your computer. Then you can simply install as ./configure --enable-float --enable-threads make sudo make install Then you can download the source code and proceed to install Gromacs in the usual way ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-float make sudo make install On occasion I have found this install to give Segmentation Faults or not work. In order to make it work it needed a little tweaking, making explicit the compiler and including additional flags, all of which may make a difference CFLAGS=\"-m64 -U_FORTIFY_SOURCE\"; ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-floats --enable-apple-64bit make sudo make install I still need to work out how to make this run in parallel on a Mac.","title":"Building Gromacs 4.* using MAKE"},{"location":"Gromacs/installation/#building-gromacs-5-using-cmake","text":"(These instructions were borrowed from Phillip W Fowler\u00b4s blog). Prerequisites for the installation are the gcc compilers available at MacPorts. This requires you to first install Xcode. Then the most important thing is to download the source code from the Gromacs website and unpack the software. tar xvf gromacs-5.0.4.tar.gz cd gromacs-5.0.4 mkdir build Then we start with the interestign stuff. In the last few versions Gromacs has made a transition towards using cmake instead of make. Cmake is readily available for Mac OS X, so no problem with this. Then you must run the following in the command line: cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX='/usr/local/gromacs/5.0.4/' make sudo make install This will install the program in /usr/local/gromacs/5.0.4 , and editing your .bashrc file you will be able to choose the version of Gromacs that is running. Adding MPI support on a Mac is trickier. This appears mainly to be because the gcc compilers from MacPorts do not appear to support OpenMPI. Here is a workaround sudo port install openmpi Now we need a compiler that supports OpenMPI sudo port install openmpi-devel-gcc49 Finally, we can follow the steps above but now we need a more complex cmake instruction cmake .. -DGMX_BUILD_OWN_FFTW=ON -DGMX_BUILD_MDRUN_ONLY=on -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/5.0.4 -DGMX_MPI=ON -DCMAKE_C_COMPILER=mpicc-openmpi-devel-gcc49 -DCMAKE_CXX_COMPILER=mpicxx-openmpi-devel-gcc49 -DGMX_SIMD=SSE4.1 make sudo make install-mdrun This is only going to build an MPI version of mdrun , as the other Gromacs programs do not run in parallel. We have to tell cmake what all the new fancy compilers are called and, unfortunately, these don\u2019t support AVX SIMD instructions so we have to fall back to SSE4.1. Experience suggests this doesn\u2019t impact performance as much as you might think. For a previous version of Gromacs (4.6.7) that also happens to be installed using cmake I have found this method not to work well. Instead I did something apparently simpler which did work. CC=mpicc CXX=mpiCC cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/4.6.7 -DGMX_MPI=ON And then one would continue with the make and make install-mdrun steps as before. This allowed for running jobs using the mpirun command as mpirun -np 12 mdrun_mpi -v $OPTIONS","title":"Building Gromacs 5.* using CMAKE"},{"location":"Gromacs/installation/#gromacs-installation-in-linux-ubuntu-server","text":"First of all we need to have everything in place for the installation. That is easily done by using Ubuntu's package manager. sudo apt-get install libibnetdisc-dev sudo apt-get install libgsl0ldbl sudo apt-get install openmpi-bin openmpi-common openssh-client openssh-server libopenmpi1.6 libopenmpi-dbg libopenmpi-dev sudo apt-get install cmake Then we dowload the relevant Gromacs version wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-5.1.2.tar.gz We decompress the file and create the build directory for running the installation tar -xvf gromacs-5.1.2.tar.gz cd gromacs-5.1.2/ mkdir build-cmake cd build-cmake/ Finally we use the appropiate flags for building the MPI version of the mdrun program and the non-MPI version of everything else sudo cmake .. -DGMX_GPU=ON -DGMX_BUILD_MDRUN_ONLY=ON -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install sudo cmake .. -DGMX_GPU=ON -DGMX_MPI=OFF -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install","title":"Gromacs installation in Linux Ubuntu Server"},{"location":"Gromacs/md_setup/","text":"Running a simulation in Gromacs These brief instructions are a general guide to how to run a simple MD simulation using Gromacs. More detailed tutorials for a variety of systems can be found elsewhere . In this case we are simulating the alanine dipeptide using Gromacs 2018 but things should not change too much for other recent versions of the software. You must have a working version installed in your machine. Get the files First of all, you will have to download the compressed files in alaTB_files.tar.gz to your computer and extract them typing tar -xvf ala_TB_files.tar.gz in your terminal. When you do this, you should be able to find a file called alaTB.gro together with a number of mdp files that we will later use. Generate the topology The next step is generating a topology file from the configuration file alaTB.gro . We will do this using the pdb2gmx program. Before we do this we will generate variables in bash with informative names which will be useful for sensible file name structure. This is what we will run in our terminal proot=\"alaTB\" ff=\"ff03\" wat=\"tip3p\" out=\"${proot}_${ff}_${wat}\" top=\"${proot}_${ff}_${wat}\" gmx pdb2gmx -f $proot -o $out -p $out <<EOF 1 1 EOF Using 1 and 1 as options after the EOF we are choosing specific options for our force field and water model (Amber03 and TIP3P respectively). These will suffice in this illustrative example but it would be advisable to do some research on your system for an educated choice of force field and water model combination. Editing the box and solvation Next we will change the size of our simulation box and we will fill it with water molecules. For this we will use two additional Gromacs programs: inp=$out out=\"${proot}_${ff}_${wat}_edit\" gmx editconf -f $inp -o $out -c -d 1 -bt cubic inp=$out out=\"${proot}_${ff}_${wat}_solv\" gmx solvate -p $top -cp $inp -o $out The first command edits the box so that the molecule is centered ( -c ) in the cubic ( -bt cubic ) simulation box and it has 1 nm between molecule and box sides ( -d 1 ). The second command simply solvates the molecule filling the box with the water model geometry of choice. Energy minimization onsidering that we are starting from a peptide conformation and a fixed water box configuration that are not particularly meaningful, we will first energy minimize the system using steepest descent. mdp=\"mini.mdp\" inp=\"${proot}_${ff}_${wat}_solv\" out=\"${proot}_${ff}_${wat}_mini\" gmx grompp -p $top -c $inp -f $mdp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out As we will do many times later, we first use the Gromacs preprocessor ( grompp ) and then run the calculation ( mdrun ). Since we are using the verbose ( -v ) option you will find that this produces a lot of output, corresponding to the gradual decrease in the energy of the system. MD in the canonical ensemble with position restraints We then equilibrate the water molecules while keeping the peptide mostly fixed using position restraints on its atoms. This is not too important in the context of a small peptide like the one we are considering, but can be critical for larger systems. mdp=\"nvt_posre.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt_posre\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out MD in the isothermal-isobaric ensemble The next step is running MD and fixing the pressure to the value of interest (typically 1 bar) with the help of a barostat. Most of the remaining parameters stay the same as were in the NVT simulation. mdp=\"npt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_npt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out Production run Finally we run a production run, again in the canonical ensemble, but now removing restraints on the peptide so that we can adequately sample conformational space. mdp=\"sd_nvt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"Simulation setup"},{"location":"Gromacs/md_setup/#running-a-simulation-in-gromacs","text":"These brief instructions are a general guide to how to run a simple MD simulation using Gromacs. More detailed tutorials for a variety of systems can be found elsewhere . In this case we are simulating the alanine dipeptide using Gromacs 2018 but things should not change too much for other recent versions of the software. You must have a working version installed in your machine.","title":"Running a simulation in Gromacs"},{"location":"Gromacs/md_setup/#get-the-files","text":"First of all, you will have to download the compressed files in alaTB_files.tar.gz to your computer and extract them typing tar -xvf ala_TB_files.tar.gz in your terminal. When you do this, you should be able to find a file called alaTB.gro together with a number of mdp files that we will later use.","title":"Get the files"},{"location":"Gromacs/md_setup/#generate-the-topology","text":"The next step is generating a topology file from the configuration file alaTB.gro . We will do this using the pdb2gmx program. Before we do this we will generate variables in bash with informative names which will be useful for sensible file name structure. This is what we will run in our terminal proot=\"alaTB\" ff=\"ff03\" wat=\"tip3p\" out=\"${proot}_${ff}_${wat}\" top=\"${proot}_${ff}_${wat}\" gmx pdb2gmx -f $proot -o $out -p $out <<EOF 1 1 EOF Using 1 and 1 as options after the EOF we are choosing specific options for our force field and water model (Amber03 and TIP3P respectively). These will suffice in this illustrative example but it would be advisable to do some research on your system for an educated choice of force field and water model combination.","title":"Generate the topology"},{"location":"Gromacs/md_setup/#editing-the-box-and-solvation","text":"Next we will change the size of our simulation box and we will fill it with water molecules. For this we will use two additional Gromacs programs: inp=$out out=\"${proot}_${ff}_${wat}_edit\" gmx editconf -f $inp -o $out -c -d 1 -bt cubic inp=$out out=\"${proot}_${ff}_${wat}_solv\" gmx solvate -p $top -cp $inp -o $out The first command edits the box so that the molecule is centered ( -c ) in the cubic ( -bt cubic ) simulation box and it has 1 nm between molecule and box sides ( -d 1 ). The second command simply solvates the molecule filling the box with the water model geometry of choice.","title":"Editing the box and solvation"},{"location":"Gromacs/md_setup/#energy-minimization","text":"onsidering that we are starting from a peptide conformation and a fixed water box configuration that are not particularly meaningful, we will first energy minimize the system using steepest descent. mdp=\"mini.mdp\" inp=\"${proot}_${ff}_${wat}_solv\" out=\"${proot}_${ff}_${wat}_mini\" gmx grompp -p $top -c $inp -f $mdp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out As we will do many times later, we first use the Gromacs preprocessor ( grompp ) and then run the calculation ( mdrun ). Since we are using the verbose ( -v ) option you will find that this produces a lot of output, corresponding to the gradual decrease in the energy of the system.","title":"Energy minimization"},{"location":"Gromacs/md_setup/#md-in-the-canonical-ensemble-with-position-restraints","text":"We then equilibrate the water molecules while keeping the peptide mostly fixed using position restraints on its atoms. This is not too important in the context of a small peptide like the one we are considering, but can be critical for larger systems. mdp=\"nvt_posre.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt_posre\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"MD in the canonical ensemble with position restraints"},{"location":"Gromacs/md_setup/#md-in-the-isothermal-isobaric-ensemble","text":"The next step is running MD and fixing the pressure to the value of interest (typically 1 bar) with the help of a barostat. Most of the remaining parameters stay the same as were in the NVT simulation. mdp=\"npt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_npt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"MD in the isothermal-isobaric ensemble"},{"location":"Gromacs/md_setup/#production-run","text":"Finally we run a production run, again in the canonical ensemble, but now removing restraints on the peptide so that we can adequately sample conformational space. mdp=\"sd_nvt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"Production run"},{"location":"Linux/admin/","text":"Linux Administration Creating and mounting new partitions In order to create a /scratch partition for my Linux server I had to first create a partition for a hard drive and then mount it in a new location. The first step needed for this was made easy by the fdisk program. Initially when running it I would get something like this username@machinename:$ sudo fdisk -l [sudo] password: Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 255 heads, 63 sectors/track, 15181 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0x00000000 Disk /dev/sdb doesn't contain a valid partition table Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux You can see that there are two drives, sda and sdb, the latter of which does not have a partition table. To create a new partition I simply typed sudo fdisk /dev/sdb and then I entered the dialogue from the fdisk program. First I created a new partition accepting most of the defaults, which resulted in the following output for fdisk -l Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 208 heads, 2 sectors/track, 586288 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0xc7a7c4cd Device Boot Start End Blocks Id System /dev/sdb1 256 243895807 975582208 83 Linux Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux Note the change in that now /dev/sdb1 appears. The next step was formating the partition, which can be achieved by typing in the command line sudo mkfs.ext4 /dev/sdb1 Then I created a mount point, which in this case is called /scratch and mounting the new partition sdb1 in that mount point. sudo mkdir /scratch sudo mount /dev/sdb1 /scratch The final bit was getting my system to mount that partition automatically, by editing the /etc/fstab file. For this I needed the UUID for my partition. This can be obtained from ls -l /dev/disk/by-uuid/ Then one must edit the /etc/fstab file with the UUID for sdb1, the mount point and the type of partition in a line looking very much like the one that follows UUID=31321321zxcgdfsdg-sdfgsdfaadsas31245 /scratch ext4 errors=remount-ro 0 2` A final detail was to recursively give read+write permissions to everyone. sudo chmod -R 777 /scratch/","title":"Linux"},{"location":"Linux/admin/#linux-administration","text":"","title":"Linux Administration"},{"location":"Linux/admin/#creating-and-mounting-new-partitions","text":"In order to create a /scratch partition for my Linux server I had to first create a partition for a hard drive and then mount it in a new location. The first step needed for this was made easy by the fdisk program. Initially when running it I would get something like this username@machinename:$ sudo fdisk -l [sudo] password: Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 255 heads, 63 sectors/track, 15181 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0x00000000 Disk /dev/sdb doesn't contain a valid partition table Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux You can see that there are two drives, sda and sdb, the latter of which does not have a partition table. To create a new partition I simply typed sudo fdisk /dev/sdb and then I entered the dialogue from the fdisk program. First I created a new partition accepting most of the defaults, which resulted in the following output for fdisk -l Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 208 heads, 2 sectors/track, 586288 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0xc7a7c4cd Device Boot Start End Blocks Id System /dev/sdb1 256 243895807 975582208 83 Linux Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux Note the change in that now /dev/sdb1 appears. The next step was formating the partition, which can be achieved by typing in the command line sudo mkfs.ext4 /dev/sdb1 Then I created a mount point, which in this case is called /scratch and mounting the new partition sdb1 in that mount point. sudo mkdir /scratch sudo mount /dev/sdb1 /scratch The final bit was getting my system to mount that partition automatically, by editing the /etc/fstab file. For this I needed the UUID for my partition. This can be obtained from ls -l /dev/disk/by-uuid/ Then one must edit the /etc/fstab file with the UUID for sdb1, the mount point and the type of partition in a line looking very much like the one that follows UUID=31321321zxcgdfsdg-sdfgsdfaadsas31245 /scratch ext4 errors=remount-ro 0 2` A final detail was to recursively give read+write permissions to everyone. sudo chmod -R 777 /scratch/","title":"Creating and mounting new partitions"},{"location":"MacOSX/hacks/","text":"Mac OS X hacks These are just a few tricks that made my life incredibly easier when transitioning from a Ubuntu Linux machine to working on a Mac desktop computer. Mounting ext2/ext3 volumes This was useful for getting my backups to be read-write-able in my Mac. This is taken from the notes that Uditha Atukorala wrote for the WireFrame 1. Install MacFUSE If you haven not already installed it download and install MacFUSE . 2. Install FUSE - Ext2 Once you have MacFUSE download and install fuse-ext2 . Even though it says fuse-ext2, this one package gives both ext2 and ext3 read-write support. After installation you should see both MacFUSE and fuse-ext2 icons in System Preferences. You now have support for ext2 and ext3 file systems. When you plug in an external ext2/ext3 partition it should automatically show up in Finder, mounted and ready to use. If auto-mount is not giving you read/write access to ext2/ext3 partitions then you will have to edit the auto-mount script for fuse-ext2 which can be found at /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util . $ sudo vi -c /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util Around line 207 (in function Mount () ) you will find the line OPTIONS=\"auto_xattr,defer_permissions\" . Change that line to read as OPTIONS=\"auto_xattr,defer_permissions,rw+\" . ... function Mount () { LogDebug \"[Mount] Entering function Mount...\" # Setting both defer_auth and defer_permissions. The option was renamed # starting with MacFUSE 1.0.0, and there seems to be no backward # compatibility on the options. # OPTIONS=\"auto_xattr,defer_permissions\" OPTIONS=\"auto_xattr,defer_permissions,rw+\" # The local option is only enabled on Leopard. It causes strange ... } This last bit was what actually solved my problem.","title":"Mac OSX"},{"location":"MacOSX/hacks/#mac-os-x-hacks","text":"These are just a few tricks that made my life incredibly easier when transitioning from a Ubuntu Linux machine to working on a Mac desktop computer.","title":"Mac OS X hacks"},{"location":"MacOSX/hacks/#mounting-ext2ext3-volumes","text":"This was useful for getting my backups to be read-write-able in my Mac. This is taken from the notes that Uditha Atukorala wrote for the WireFrame","title":"Mounting ext2/ext3 volumes"},{"location":"MacOSX/hacks/#1-install-macfuse","text":"If you haven not already installed it download and install MacFUSE .","title":"1. Install MacFUSE"},{"location":"MacOSX/hacks/#2-install-fuse-ext2","text":"Once you have MacFUSE download and install fuse-ext2 . Even though it says fuse-ext2, this one package gives both ext2 and ext3 read-write support. After installation you should see both MacFUSE and fuse-ext2 icons in System Preferences. You now have support for ext2 and ext3 file systems. When you plug in an external ext2/ext3 partition it should automatically show up in Finder, mounted and ready to use. If auto-mount is not giving you read/write access to ext2/ext3 partitions then you will have to edit the auto-mount script for fuse-ext2 which can be found at /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util . $ sudo vi -c /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util Around line 207 (in function Mount () ) you will find the line OPTIONS=\"auto_xattr,defer_permissions\" . Change that line to read as OPTIONS=\"auto_xattr,defer_permissions,rw+\" . ... function Mount () { LogDebug \"[Mount] Entering function Mount...\" # Setting both defer_auth and defer_permissions. The option was renamed # starting with MacFUSE 1.0.0, and there seems to be no backward # compatibility on the options. # OPTIONS=\"auto_xattr,defer_permissions\" OPTIONS=\"auto_xattr,defer_permissions,rw+\" # The local option is only enabled on Leopard. It causes strange ... } This last bit was what actually solved my problem.","title":"2. Install FUSE - Ext2"},{"location":"Other/other/","text":"Advice on writing scientific papers The Art of Writing Science, by Kevin W. Plaxco ( Protein Sci., 2010 ). Whitesides' Group: Writing a Paper, by G. M. Whitesides ( Adv. Materials, 2004 ).","title":"Other"},{"location":"Other/other/#advice-on-writing-scientific-papers","text":"The Art of Writing Science, by Kevin W. Plaxco ( Protein Sci., 2010 ). Whitesides' Group: Writing a Paper, by G. M. Whitesides ( Adv. Materials, 2004 ).","title":"Advice on writing scientific papers"},{"location":"PBS/documentation/","text":"Installing Torque Bellow follow instructions on how to install Torque in a multiprocessor Ubuntu Linux server. In this case the same machine is used as server, scheduler, submission node and compute node. These notes have been borrowed from this blog post (thanks!) and are kept here for future records only. The version of Ubuntu used in this case was 14.04 LTS. The first thing to note is that you should do all of these as root . Then we must ensure that the first line in the /etc/hosts file reads as follows 127.0.0.1 localhost Next comes the installation of some packages, which we do using Ubuntu`s package manager. apt-get install torque-server torque-client torque-mom torque-pam After this step we simply stop these services since, apparently, the initial torque configuration does not really work as one would hope. In order to achieve this we simply type the following in the terminal /etc/init.d/torque-mom stop /etc/init.d/torque-scheduler stop /etc/init.d/torque-server stop We then can create a new setup for torque using the following pbs_server -t create When prompted about whether we want to overwrite the existing database we will reply yes ( [y] ). Next the just-started server instance is killed using the following command for further configuration killall pbs_server Next we will set up the server process. In my case the server is simply called localhost and I experienced some problems when trying to use a different server domain. echo localhost > /etc/torque/server_name echo localhost > /var/spool/torque/server_priv/acl_svr/acl_hosts echo root@localhost > /var/spool/torque/server_priv/acl_svr/operators echo root@localhost > /var/spool/torque/server_priv/acl_svr/managers The following step is to simply add the compute nodes. Since here we are using the \"head node\" as \"compute node\" then we just need to type the following echo \"localhost np=56\" > /var/spool/torque/server_priv/nodes Then we start the MOM process that handles the compute node echo localhost > /var/spool/torque/mom_priv/config After all of these one has to restart the processes again /etc/init.d/torque-server start /etc/init.d/torque-scheduler start /etc/init.d/torque-mom start Finally we need to restart the scheduler, create the default queue and configure thee server to allow submissions from itself qmgr -c \"set server scheduling = true\" qmgr -c \"set server keep_completed = 300\" qmgr -c \"set server mom_job_sync = true\" # create default queue qmgr -c \"create queue batch\" qmgr -c \"set queue batch queue_type = execution\" qmgr -c \"set queue batch started = true\" qmgr -c \"set queue batch enabled = true\" qmgr -c \"set queue batch resources_default.walltime = 1:00:00\" qmgr -c \"set queue batch resources_default.nodes = 1\" qmgr -c \"set server default_queue = batch\" # configure submission pool qmgr -c \"set server submit_hosts = localhost\" qmgr -c \"set server allow_node_submit = true\" Finally you can test whether everything is working right for you using the following command qsub -I An additional test script that can be done is to run this simples PBS script test.sh #!/bin/bash cd $PBS_O_WORKDIR #direct the output to cluster_nodes cat $PBS_NODEFILE > ./cluster_nodes This should run by simply writing the following command on your terminal qsub test.sh Example PBS script This is just an example PBS script for submitting jobs in the Archer supercomputing facility. #!/bin/bash --login #PBS -N jobname # Select 1 node #PBS -l select=1 #PBS -l walltime=24:00:00 #PBS -m abe #PBS -M name@emailprovider.org # Replace this with your budget code #PBS -A budget # Move to directory that script was submitted from #export PBS_O_WORKDIR=$(readlink -f $PBS_O_WORKDIR) #echo $PBS_O_WORKDIR #exit #cd $PBS_O_WORKDIR cd \"/work/directory/\" # Load the GROMACS module module add gromacs # Run GROMACS using default input and output file names pull=1 k=100 options=\"-s ${file} -deffnm ${file}\" aprun -n 24 mdrun_mpi $options","title":"PBS"},{"location":"PBS/documentation/#installing-torque","text":"Bellow follow instructions on how to install Torque in a multiprocessor Ubuntu Linux server. In this case the same machine is used as server, scheduler, submission node and compute node. These notes have been borrowed from this blog post (thanks!) and are kept here for future records only. The version of Ubuntu used in this case was 14.04 LTS. The first thing to note is that you should do all of these as root . Then we must ensure that the first line in the /etc/hosts file reads as follows 127.0.0.1 localhost Next comes the installation of some packages, which we do using Ubuntu`s package manager. apt-get install torque-server torque-client torque-mom torque-pam After this step we simply stop these services since, apparently, the initial torque configuration does not really work as one would hope. In order to achieve this we simply type the following in the terminal /etc/init.d/torque-mom stop /etc/init.d/torque-scheduler stop /etc/init.d/torque-server stop We then can create a new setup for torque using the following pbs_server -t create When prompted about whether we want to overwrite the existing database we will reply yes ( [y] ). Next the just-started server instance is killed using the following command for further configuration killall pbs_server Next we will set up the server process. In my case the server is simply called localhost and I experienced some problems when trying to use a different server domain. echo localhost > /etc/torque/server_name echo localhost > /var/spool/torque/server_priv/acl_svr/acl_hosts echo root@localhost > /var/spool/torque/server_priv/acl_svr/operators echo root@localhost > /var/spool/torque/server_priv/acl_svr/managers The following step is to simply add the compute nodes. Since here we are using the \"head node\" as \"compute node\" then we just need to type the following echo \"localhost np=56\" > /var/spool/torque/server_priv/nodes Then we start the MOM process that handles the compute node echo localhost > /var/spool/torque/mom_priv/config After all of these one has to restart the processes again /etc/init.d/torque-server start /etc/init.d/torque-scheduler start /etc/init.d/torque-mom start Finally we need to restart the scheduler, create the default queue and configure thee server to allow submissions from itself qmgr -c \"set server scheduling = true\" qmgr -c \"set server keep_completed = 300\" qmgr -c \"set server mom_job_sync = true\" # create default queue qmgr -c \"create queue batch\" qmgr -c \"set queue batch queue_type = execution\" qmgr -c \"set queue batch started = true\" qmgr -c \"set queue batch enabled = true\" qmgr -c \"set queue batch resources_default.walltime = 1:00:00\" qmgr -c \"set queue batch resources_default.nodes = 1\" qmgr -c \"set server default_queue = batch\" # configure submission pool qmgr -c \"set server submit_hosts = localhost\" qmgr -c \"set server allow_node_submit = true\" Finally you can test whether everything is working right for you using the following command qsub -I An additional test script that can be done is to run this simples PBS script test.sh #!/bin/bash cd $PBS_O_WORKDIR #direct the output to cluster_nodes cat $PBS_NODEFILE > ./cluster_nodes This should run by simply writing the following command on your terminal qsub test.sh","title":"Installing Torque"},{"location":"PBS/documentation/#example-pbs-script","text":"This is just an example PBS script for submitting jobs in the Archer supercomputing facility. #!/bin/bash --login #PBS -N jobname # Select 1 node #PBS -l select=1 #PBS -l walltime=24:00:00 #PBS -m abe #PBS -M name@emailprovider.org # Replace this with your budget code #PBS -A budget # Move to directory that script was submitted from #export PBS_O_WORKDIR=$(readlink -f $PBS_O_WORKDIR) #echo $PBS_O_WORKDIR #exit #cd $PBS_O_WORKDIR cd \"/work/directory/\" # Load the GROMACS module module add gromacs # Run GROMACS using default input and output file names pull=1 k=100 options=\"-s ${file} -deffnm ${file}\" aprun -n 24 mdrun_mpi $options","title":"Example PBS script"},{"location":"VMD/tools/","text":"VMD scripting It is relatively easy to use the command line instead of the GUI in VMD. This is particularly useful when one needs to load multiple molecules, or make multiple visualizations look the same (e.g. making multiple molecules look like NewCartoon but changing their colour). Loading a molecule mol new file.pdb Applying a visualization style mol modstyle 0 0 NewCartoon Changing colour mol modcolor 0 0 ColorID 2 Loading a trajectory mol addfile traj_comp.xtc Loops in VMD The example below is for changing the colour of five different molecules so that each has a different colour. for {set x 0} {$x <= 5} {incr x} { mol modcolor 0 $x ColorID $x } Making videos in VMD VMD has a built in tool called Movie Maker. You must use it to generate a video according to your particular taste. You can find some instructions here . One possibility is that you save the files for each of the snapshots of the simulation, instead of letting VMD produce the movie and remove the files for you. If you do not do that, then you can tweak things a bit, in terms of using different movie formats or tuning the bitrate. Below is an example of the command that you can invoke for generating the video. ffmpeg -i filename.%05d.ppm -r 25 -an -b 10000k -bt 10000k moviename.mpg Clearly, you will need the ffmpeg program for doing this, which in a Mac you can obtain from MacPorts. Then the %0.5d in the filename just corresponds to the different files written by VMD that you want to process (be careful with format).","title":"VMD"},{"location":"VMD/tools/#vmd-scripting","text":"It is relatively easy to use the command line instead of the GUI in VMD. This is particularly useful when one needs to load multiple molecules, or make multiple visualizations look the same (e.g. making multiple molecules look like NewCartoon but changing their colour).","title":"VMD scripting"},{"location":"VMD/tools/#loading-a-molecule","text":"mol new file.pdb","title":"Loading a molecule"},{"location":"VMD/tools/#applying-a-visualization-style","text":"mol modstyle 0 0 NewCartoon","title":"Applying a visualization style"},{"location":"VMD/tools/#changing-colour","text":"mol modcolor 0 0 ColorID 2","title":"Changing colour"},{"location":"VMD/tools/#loading-a-trajectory","text":"mol addfile traj_comp.xtc","title":"Loading a trajectory"},{"location":"VMD/tools/#loops-in-vmd","text":"The example below is for changing the colour of five different molecules so that each has a different colour. for {set x 0} {$x <= 5} {incr x} { mol modcolor 0 $x ColorID $x }","title":"Loops in VMD"},{"location":"VMD/tools/#making-videos-in-vmd","text":"VMD has a built in tool called Movie Maker. You must use it to generate a video according to your particular taste. You can find some instructions here . One possibility is that you save the files for each of the snapshots of the simulation, instead of letting VMD produce the movie and remove the files for you. If you do not do that, then you can tweak things a bit, in terms of using different movie formats or tuning the bitrate. Below is an example of the command that you can invoke for generating the video. ffmpeg -i filename.%05d.ppm -r 25 -an -b 10000k -bt 10000k moviename.mpg Clearly, you will need the ffmpeg program for doing this, which in a Mac you can obtain from MacPorts. Then the %0.5d in the filename just corresponds to the different files written by VMD that you want to process (be careful with format).","title":"Making videos in VMD"}]}