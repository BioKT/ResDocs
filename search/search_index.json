{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"BioKT Research Toolbox This is the documentation website for the BioKT group at the University of the Basque Country and the Donostia International Physics Center . We are a team lead by Xabier Lopez and David De Sancho that focuses on the study of the conformational dynamics of biomolecules with computational methods, in particular molecular simulation and quantum mechanical calculations. You can find more information about the work we do in our website . Much of the code we write is hosted in Github . In this website you will find information about a number of common tools that we use in our research.","title":"Home"},{"location":"index.html#biokt-research-toolbox","text":"This is the documentation website for the BioKT group at the University of the Basque Country and the Donostia International Physics Center . We are a team lead by Xabier Lopez and David De Sancho that focuses on the study of the conformational dynamics of biomolecules with computational methods, in particular molecular simulation and quantum mechanical calculations. You can find more information about the work we do in our website . Much of the code we write is hosted in Github . In this website you will find information about a number of common tools that we use in our research.","title":"BioKT Research Toolbox"},{"location":"Amber/peptide.html","text":"Building molecules with tleap In order to build PDB structures from sequence one may use programs like LEaP , included with Ambertools. The example below corresponds to the specific case of an acetylated and amidated alanine pentapeptide. We start by starting LEaP , which in this specific case was installed through ananconda $> tleap -I: Adding /Users/.../anaconda3/dat/leap/prep to search path. -I: Adding /Users/.../anaconda3/dat/leap/lib to search path. -I: Adding /Users/.../anaconda3/dat/leap/parm to search path. -I: Adding /Users/.../anaconda3/dat/leap/cmd to search path. Welcome to LEaP! (no leaprc in search path) This will leave a prompt for you to type a series of commands. Next we define the sequence of the peptide > source leaprc.protein.ff14SBonlysc ----- Source: /Users/.../anaconda3/dat/leap/cmd/leaprc.protein.ff14SBonlysc ----- Source of /Users/.../anaconda3/dat/leap/cmd/leaprc.protein.ff14SBonlysc done Log file: ./leap.log Loading parameters: /Users/.../anaconda3/dat/leap/parm/parm10.dat Reading title: PARM99 + frcmod.ff99SB + frcmod.parmbsc0 + OL3 for RNA Loading parameters: /Users/.../anaconda3/dat/leap/parm/frcmod.ff14SB Reading force field modification type file (frcmod) Reading title: ff14SB protein backbone and sidechain parameters Loading parameters: /Users/.../anaconda3/dat/leap/parm/frcmod.ff99SB14 Reading force field modification type file (frcmod) Reading title: ff99SB backbone parameters (Hornak & Simmerling) with ff14SB atom types Loading library: /Users/.../anaconda3/dat/leap/lib/amino12.lib Loading library: /Users/.../anaconda3/dat/leap/lib/aminoct12.lib Loading library: /Users/.../anaconda3/dat/leap/lib/aminont12.lib Then we specify the sequence and whether we want to define a helical conformation. > ala5 = sequence {ACE ALA ALA ALA ALA ALA NHE} > impose ala5 { 1 2 3 4 5 } {{ \"N\" \"CA\" \"C\" \"N\" -40} {\"C\" \"N\" \"CA\" \"C\" -60}} Finally, we can save the structure to a PDB file > savepdb ala5 ala5.pdb Writing pdb file: ala5.pdb Adding D-amino acids to proteins Following the example shown above, lets consider that the 2nd ALA is a D-amino acid rather than the defaul L-amino acid. In order to add the D-amino acid, after the sequence is built, we flip the selected residue. To do so, we define the sequence name, ala5 in this case, number of residue to flip and the atom we want to flip CA . Lastly, we flip the selected amino acid as shown below: select ala5.2.CA flip ala5 then, we continue as described above. Building a topology file Having built (or downloaded from the internet) a PDB file, we can proceed to generate other files required to run a simulation in Amber. Instead of invoking commands one by one, we can write an input file for LEaP Open your text editor and write the following #tleap.in source leaprc.protein.ff19SB source leaprc.water.opc ala5=loadpdb ala5.amber.pdb SaveAmberParm ala5 ala5_gas.prmtop ala5_gas.inpcrd solvateOct ala5 OPCBOX 5.0 addIonsRand ala5 Na+ 10 Cl- 10 SaveAmberParm ala5 ala5_solv.prmtop ala5_solv.inpcrd Now save it as tleap.inp . Then, we can run the following command in the command line $ tleap -f tleap.inp This will result in a fully solvated simulation box. You may want to save it as a PDB file. In order to do this, you can run a write an input file for cpptraj . Use your text editor to write the following trajin ala5_solv.inpcrd trajout ala5_water.pdb PDB run and save it as pdb.inp . Now run $ cpptraj -p ala5_solv.prmtop -i pdb.inp For more details, check the official documentation of Amber here .","title":"Building molecules"},{"location":"Amber/peptide.html#building-molecules-with-tleap","text":"In order to build PDB structures from sequence one may use programs like LEaP , included with Ambertools. The example below corresponds to the specific case of an acetylated and amidated alanine pentapeptide. We start by starting LEaP , which in this specific case was installed through ananconda $> tleap -I: Adding /Users/.../anaconda3/dat/leap/prep to search path. -I: Adding /Users/.../anaconda3/dat/leap/lib to search path. -I: Adding /Users/.../anaconda3/dat/leap/parm to search path. -I: Adding /Users/.../anaconda3/dat/leap/cmd to search path. Welcome to LEaP! (no leaprc in search path) This will leave a prompt for you to type a series of commands. Next we define the sequence of the peptide > source leaprc.protein.ff14SBonlysc ----- Source: /Users/.../anaconda3/dat/leap/cmd/leaprc.protein.ff14SBonlysc ----- Source of /Users/.../anaconda3/dat/leap/cmd/leaprc.protein.ff14SBonlysc done Log file: ./leap.log Loading parameters: /Users/.../anaconda3/dat/leap/parm/parm10.dat Reading title: PARM99 + frcmod.ff99SB + frcmod.parmbsc0 + OL3 for RNA Loading parameters: /Users/.../anaconda3/dat/leap/parm/frcmod.ff14SB Reading force field modification type file (frcmod) Reading title: ff14SB protein backbone and sidechain parameters Loading parameters: /Users/.../anaconda3/dat/leap/parm/frcmod.ff99SB14 Reading force field modification type file (frcmod) Reading title: ff99SB backbone parameters (Hornak & Simmerling) with ff14SB atom types Loading library: /Users/.../anaconda3/dat/leap/lib/amino12.lib Loading library: /Users/.../anaconda3/dat/leap/lib/aminoct12.lib Loading library: /Users/.../anaconda3/dat/leap/lib/aminont12.lib Then we specify the sequence and whether we want to define a helical conformation. > ala5 = sequence {ACE ALA ALA ALA ALA ALA NHE} > impose ala5 { 1 2 3 4 5 } {{ \"N\" \"CA\" \"C\" \"N\" -40} {\"C\" \"N\" \"CA\" \"C\" -60}} Finally, we can save the structure to a PDB file > savepdb ala5 ala5.pdb Writing pdb file: ala5.pdb","title":"Building molecules with tleap"},{"location":"Amber/peptide.html#adding-d-amino-acids-to-proteins","text":"Following the example shown above, lets consider that the 2nd ALA is a D-amino acid rather than the defaul L-amino acid. In order to add the D-amino acid, after the sequence is built, we flip the selected residue. To do so, we define the sequence name, ala5 in this case, number of residue to flip and the atom we want to flip CA . Lastly, we flip the selected amino acid as shown below: select ala5.2.CA flip ala5 then, we continue as described above.","title":"Adding D-amino acids to proteins"},{"location":"Amber/peptide.html#building-a-topology-file","text":"Having built (or downloaded from the internet) a PDB file, we can proceed to generate other files required to run a simulation in Amber. Instead of invoking commands one by one, we can write an input file for LEaP Open your text editor and write the following #tleap.in source leaprc.protein.ff19SB source leaprc.water.opc ala5=loadpdb ala5.amber.pdb SaveAmberParm ala5 ala5_gas.prmtop ala5_gas.inpcrd solvateOct ala5 OPCBOX 5.0 addIonsRand ala5 Na+ 10 Cl- 10 SaveAmberParm ala5 ala5_solv.prmtop ala5_solv.inpcrd Now save it as tleap.inp . Then, we can run the following command in the command line $ tleap -f tleap.inp This will result in a fully solvated simulation box. You may want to save it as a PDB file. In order to do this, you can run a write an input file for cpptraj . Use your text editor to write the following trajin ala5_solv.inpcrd trajout ala5_water.pdb PDB run and save it as pdb.inp . Now run $ cpptraj -p ala5_solv.prmtop -i pdb.inp For more details, check the official documentation of Amber here .","title":"Building a topology file"},{"location":"Beast/installation.html","text":"Beast and Beagle installation in a Linux server These instructions are valid for a Linux server running Ubuntu 14.04 LTS, with 2 Intel Xeon processors and 2 Nvidia GPUs. They are are based on those found in the Beast and Beagle sites, plus my own experience. The first thing I had to do was to get all the dependencies right, for which I run the following sudo apt-get install build-essential autoconf automake libtool subversion pkg-config openjdk-6-jdk Once these were all in place, I tried using the NVIDIA OpenCL implementation which was accessible via the apt-get command. However, I was unable to install the Beagle library so that it was recognized. Tests systematically failed. Hence, I resorted to the Intel implementation of OpenCL . After the download, this can be easily installed cd opencl_runtime_16.1.1_x64_ubuntu_6.4.0.25/ sudo bash install.sh Then I went back to the installation of the Beagle library. ./autogen.sh ./configure --prefix=\"/opt/beast/beagle-lib\" --with-opencl=\"/opt/intel/opencl/lib64\" sudo make install In order to check whether I was getting the correct functionality I run the test suite make check Finally all went well. So that allowed running the downloaded Beast executable available here .","title":"Beast"},{"location":"Beast/installation.html#beast-and-beagle-installation-in-a-linux-server","text":"These instructions are valid for a Linux server running Ubuntu 14.04 LTS, with 2 Intel Xeon processors and 2 Nvidia GPUs. They are are based on those found in the Beast and Beagle sites, plus my own experience. The first thing I had to do was to get all the dependencies right, for which I run the following sudo apt-get install build-essential autoconf automake libtool subversion pkg-config openjdk-6-jdk Once these were all in place, I tried using the NVIDIA OpenCL implementation which was accessible via the apt-get command. However, I was unable to install the Beagle library so that it was recognized. Tests systematically failed. Hence, I resorted to the Intel implementation of OpenCL . After the download, this can be easily installed cd opencl_runtime_16.1.1_x64_ubuntu_6.4.0.25/ sudo bash install.sh Then I went back to the installation of the Beagle library. ./autogen.sh ./configure --prefix=\"/opt/beast/beagle-lib\" --with-opencl=\"/opt/intel/opencl/lib64\" sudo make install In order to check whether I was getting the correct functionality I run the test suite make check Finally all went well. So that allowed running the downloaded Beast executable available here .","title":"Beast and Beagle installation in a Linux server"},{"location":"Gromacs/benchmarks.html","text":"Gromacs benchmarks in our systems Hydrogenase This system consists of a protein with 19054 water molecules plus ions, summing a total of 83764 atoms. We have run simulations in both the ATLAS and ARINA clusters.","title":"Benchmarks"},{"location":"Gromacs/benchmarks.html#gromacs-benchmarks-in-our-systems","text":"","title":"Gromacs benchmarks in our systems"},{"location":"Gromacs/benchmarks.html#hydrogenase","text":"This system consists of a protein with 19054 water molecules plus ions, summing a total of 83764 atoms. We have run simulations in both the ATLAS and ARINA clusters.","title":"Hydrogenase"},{"location":"Gromacs/installation.html","text":"Gromacs installation in Mac OS X Building Gromacs 4.* using MAKE One of the prerequisites for the installation are the fftw libraries for doing Fourier transforms. Setting these up correctly seems to be limiting, as in the configure step Gromacs struggled to find the Macports libraries. So first of all, download fftw-3.0.1.tar.gz on your computer. Then you can simply install as ./configure --enable-float --enable-threads make sudo make install Then you can download the source code and proceed to install Gromacs in the usual way ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-float make sudo make install On occasion I have found this install to give Segmentation Faults or not work. In order to make it work it needed a little tweaking, making explicit the compiler and including additional flags, all of which may make a difference CFLAGS=\"-m64 -U_FORTIFY_SOURCE\"; ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-floats --enable-apple-64bit make sudo make install I still need to work out how to make this run in parallel on a Mac. Building Gromacs 5.* using CMAKE (These instructions were borrowed from Phillip W Fowler\u00b4s blog). Prerequisites for the installation are the gcc compilers available at MacPorts. This requires you to first install Xcode. Then the most important thing is to download the source code from the Gromacs website and unpack the software. tar xvf gromacs-5.0.4.tar.gz cd gromacs-5.0.4 mkdir build Then we start with the interestign stuff. In the last few versions Gromacs has made a transition towards using cmake instead of make. Cmake is readily available for Mac OS X, so no problem with this. Then you must run the following in the command line: cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX='/usr/local/gromacs/5.0.4/' make sudo make install This will install the program in /usr/local/gromacs/5.0.4 , and editing your .bashrc file you will be able to choose the version of Gromacs that is running. Adding MPI support on a Mac is trickier. This appears mainly to be because the gcc compilers from MacPorts do not appear to support OpenMPI. Here is a workaround sudo port install openmpi Now we need a compiler that supports OpenMPI sudo port install openmpi-devel-gcc49 Finally, we can follow the steps above but now we need a more complex cmake instruction cmake .. -DGMX_BUILD_OWN_FFTW=ON -DGMX_BUILD_MDRUN_ONLY=on -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/5.0.4 -DGMX_MPI=ON -DCMAKE_C_COMPILER=mpicc-openmpi-devel-gcc49 -DCMAKE_CXX_COMPILER=mpicxx-openmpi-devel-gcc49 -DGMX_SIMD=SSE4.1 make sudo make install-mdrun This is only going to build an MPI version of mdrun , as the other Gromacs programs do not run in parallel. We have to tell cmake what all the new fancy compilers are called and, unfortunately, these don\u2019t support AVX SIMD instructions so we have to fall back to SSE4.1. Experience suggests this doesn\u2019t impact performance as much as you might think. For a previous version of Gromacs (4.6.7) that also happens to be installed using cmake I have found this method not to work well. Instead I did something apparently simpler which did work. CC=mpicc CXX=mpiCC cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/4.6.7 -DGMX_MPI=ON And then one would continue with the make and make install-mdrun steps as before. This allowed for running jobs using the mpirun command as mpirun -np 12 mdrun_mpi -v $OPTIONS Gromacs installation in Linux Ubuntu Server First of all we need to have everything in place for the installation. That is easily done by using Ubuntu's package manager. sudo apt-get install libibnetdisc-dev sudo apt-get install libgsl0ldbl sudo apt-get install openmpi-bin openmpi-common openssh-client openssh-server libopenmpi1.6 libopenmpi-dbg libopenmpi-dev sudo apt-get install cmake Then we dowload the relevant Gromacs version wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-5.1.2.tar.gz We decompress the file and create the build directory for running the installation tar -xvf gromacs-5.1.2.tar.gz cd gromacs-5.1.2/ mkdir build-cmake cd build-cmake/ Finally we use the appropiate flags for building the MPI version of the mdrun program and the non-MPI version of everything else sudo cmake .. -DGMX_GPU=ON -DGMX_BUILD_MDRUN_ONLY=ON -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install sudo cmake .. -DGMX_GPU=ON -DGMX_MPI=OFF -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install","title":"Installation"},{"location":"Gromacs/installation.html#gromacs-installation-in-mac-os-x","text":"","title":"Gromacs installation in Mac OS X"},{"location":"Gromacs/installation.html#building-gromacs-4-using-make","text":"One of the prerequisites for the installation are the fftw libraries for doing Fourier transforms. Setting these up correctly seems to be limiting, as in the configure step Gromacs struggled to find the Macports libraries. So first of all, download fftw-3.0.1.tar.gz on your computer. Then you can simply install as ./configure --enable-float --enable-threads make sudo make install Then you can download the source code and proceed to install Gromacs in the usual way ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-float make sudo make install On occasion I have found this install to give Segmentation Faults or not work. In order to make it work it needed a little tweaking, making explicit the compiler and including additional flags, all of which may make a difference CFLAGS=\"-m64 -U_FORTIFY_SOURCE\"; ./configure --prefix=/usr/local/gromacs/4.0.5 --enable-threads --enable-floats --enable-apple-64bit make sudo make install I still need to work out how to make this run in parallel on a Mac.","title":"Building Gromacs 4.* using MAKE"},{"location":"Gromacs/installation.html#building-gromacs-5-using-cmake","text":"(These instructions were borrowed from Phillip W Fowler\u00b4s blog). Prerequisites for the installation are the gcc compilers available at MacPorts. This requires you to first install Xcode. Then the most important thing is to download the source code from the Gromacs website and unpack the software. tar xvf gromacs-5.0.4.tar.gz cd gromacs-5.0.4 mkdir build Then we start with the interestign stuff. In the last few versions Gromacs has made a transition towards using cmake instead of make. Cmake is readily available for Mac OS X, so no problem with this. Then you must run the following in the command line: cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX='/usr/local/gromacs/5.0.4/' make sudo make install This will install the program in /usr/local/gromacs/5.0.4 , and editing your .bashrc file you will be able to choose the version of Gromacs that is running. Adding MPI support on a Mac is trickier. This appears mainly to be because the gcc compilers from MacPorts do not appear to support OpenMPI. Here is a workaround sudo port install openmpi Now we need a compiler that supports OpenMPI sudo port install openmpi-devel-gcc49 Finally, we can follow the steps above but now we need a more complex cmake instruction cmake .. -DGMX_BUILD_OWN_FFTW=ON -DGMX_BUILD_MDRUN_ONLY=on -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/5.0.4 -DGMX_MPI=ON -DCMAKE_C_COMPILER=mpicc-openmpi-devel-gcc49 -DCMAKE_CXX_COMPILER=mpicxx-openmpi-devel-gcc49 -DGMX_SIMD=SSE4.1 make sudo make install-mdrun This is only going to build an MPI version of mdrun , as the other Gromacs programs do not run in parallel. We have to tell cmake what all the new fancy compilers are called and, unfortunately, these don\u2019t support AVX SIMD instructions so we have to fall back to SSE4.1. Experience suggests this doesn\u2019t impact performance as much as you might think. For a previous version of Gromacs (4.6.7) that also happens to be installed using cmake I have found this method not to work well. Instead I did something apparently simpler which did work. CC=mpicc CXX=mpiCC cmake .. -DGMX_BUILD_OWN_FFTW=ON -DCMAKE_INSTALL_PREFIX=/usr/local/gromacs/4.6.7 -DGMX_MPI=ON And then one would continue with the make and make install-mdrun steps as before. This allowed for running jobs using the mpirun command as mpirun -np 12 mdrun_mpi -v $OPTIONS","title":"Building Gromacs 5.* using CMAKE"},{"location":"Gromacs/installation.html#gromacs-installation-in-linux-ubuntu-server","text":"First of all we need to have everything in place for the installation. That is easily done by using Ubuntu's package manager. sudo apt-get install libibnetdisc-dev sudo apt-get install libgsl0ldbl sudo apt-get install openmpi-bin openmpi-common openssh-client openssh-server libopenmpi1.6 libopenmpi-dbg libopenmpi-dev sudo apt-get install cmake Then we dowload the relevant Gromacs version wget ftp://ftp.gromacs.org/pub/gromacs/gromacs-5.1.2.tar.gz We decompress the file and create the build directory for running the installation tar -xvf gromacs-5.1.2.tar.gz cd gromacs-5.1.2/ mkdir build-cmake cd build-cmake/ Finally we use the appropiate flags for building the MPI version of the mdrun program and the non-MPI version of everything else sudo cmake .. -DGMX_GPU=ON -DGMX_BUILD_MDRUN_ONLY=ON -DGMX_MPI=ON -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install sudo cmake .. -DGMX_GPU=ON -DGMX_MPI=OFF -DCMAKE_INSTALL_PREFIX=/opt/gromacs/5.1.2 sudo make install","title":"Gromacs installation in Linux Ubuntu Server"},{"location":"Gromacs/intro.html","text":"Gromacs is the molecular simulation package that we most often use. It's ``fast, flexible and free'' and it is extremely well documented.","title":"Intro"},{"location":"Gromacs/md_setup.html","text":"Running a simulation in Gromacs These brief instructions are a general guide to how to run a simple MD simulation using Gromacs. More detailed tutorials for a variety of systems can be found elsewhere . In this case we are simulating the alanine dipeptide using Gromacs 2018 but things should not change too much for other recent versions of the software. You must have a working version installed in your machine. Get the files First of all, you will have to download the compressed files in alaTB_files.tar.gz to your computer and extract them typing tar -xvf ala_TB_files.tar.gz in your terminal. When you do this, you should be able to find a file called alaTB.gro together with a number of mdp files that we will later use. Generate the topology The next step is generating a topology file from the configuration file alaTB.gro . We will do this using the pdb2gmx program. Before we do this we will generate variables in bash with informative names which will be useful for sensible file name structure. This is what we will run in our terminal proot=\"alaTB\" ff=\"ff03\" wat=\"tip3p\" out=\"${proot}_${ff}_${wat}\" top=\"${proot}_${ff}_${wat}\" gmx pdb2gmx -f $proot -o $out -p $out <<EOF 1 1 EOF Using 1 and 1 as options after the EOF we are choosing specific options for our force field and water model (Amber03 and TIP3P respectively). These will suffice in this illustrative example but it would be advisable to do some research on your system for an educated choice of force field and water model combination. Editing the box and solvation Next we will change the size of our simulation box and we will fill it with water molecules. For this we will use two additional Gromacs programs: inp=$out out=\"${proot}_${ff}_${wat}_edit\" gmx editconf -f $inp -o $out -c -d 1 -bt cubic inp=$out out=\"${proot}_${ff}_${wat}_solv\" gmx solvate -p $top -cp $inp -o $out The first command edits the box so that the molecule is centered ( -c ) in the cubic ( -bt cubic ) simulation box and it has 1 nm between molecule and box sides ( -d 1 ). The second command simply solvates the molecule filling the box with the water model geometry of choice. Energy minimization onsidering that we are starting from a peptide conformation and a fixed water box configuration that are not particularly meaningful, we will first energy minimize the system using steepest descent. mdp=\"mini.mdp\" inp=\"${proot}_${ff}_${wat}_solv\" out=\"${proot}_${ff}_${wat}_mini\" gmx grompp -p $top -c $inp -f $mdp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out As we will do many times later, we first use the Gromacs preprocessor ( grompp ) and then run the calculation ( mdrun ). Since we are using the verbose ( -v ) option you will find that this produces a lot of output, corresponding to the gradual decrease in the energy of the system. MD in the canonical ensemble with position restraints We then equilibrate the water molecules while keeping the peptide mostly fixed using position restraints on its atoms. This is not too important in the context of a small peptide like the one we are considering, but can be critical for larger systems. mdp=\"nvt_posre.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt_posre\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out MD in the isothermal-isobaric ensemble The next step is running MD and fixing the pressure to the value of interest (typically 1 bar) with the help of a barostat. Most of the remaining parameters stay the same as were in the NVT simulation. mdp=\"npt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_npt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out Production run Finally we run a production run, again in the canonical ensemble, but now removing restraints on the peptide so that we can adequately sample conformational space. mdp=\"sd_nvt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"Simulation setup"},{"location":"Gromacs/md_setup.html#running-a-simulation-in-gromacs","text":"These brief instructions are a general guide to how to run a simple MD simulation using Gromacs. More detailed tutorials for a variety of systems can be found elsewhere . In this case we are simulating the alanine dipeptide using Gromacs 2018 but things should not change too much for other recent versions of the software. You must have a working version installed in your machine.","title":"Running a simulation in Gromacs"},{"location":"Gromacs/md_setup.html#get-the-files","text":"First of all, you will have to download the compressed files in alaTB_files.tar.gz to your computer and extract them typing tar -xvf ala_TB_files.tar.gz in your terminal. When you do this, you should be able to find a file called alaTB.gro together with a number of mdp files that we will later use.","title":"Get the files"},{"location":"Gromacs/md_setup.html#generate-the-topology","text":"The next step is generating a topology file from the configuration file alaTB.gro . We will do this using the pdb2gmx program. Before we do this we will generate variables in bash with informative names which will be useful for sensible file name structure. This is what we will run in our terminal proot=\"alaTB\" ff=\"ff03\" wat=\"tip3p\" out=\"${proot}_${ff}_${wat}\" top=\"${proot}_${ff}_${wat}\" gmx pdb2gmx -f $proot -o $out -p $out <<EOF 1 1 EOF Using 1 and 1 as options after the EOF we are choosing specific options for our force field and water model (Amber03 and TIP3P respectively). These will suffice in this illustrative example but it would be advisable to do some research on your system for an educated choice of force field and water model combination.","title":"Generate the topology"},{"location":"Gromacs/md_setup.html#editing-the-box-and-solvation","text":"Next we will change the size of our simulation box and we will fill it with water molecules. For this we will use two additional Gromacs programs: inp=$out out=\"${proot}_${ff}_${wat}_edit\" gmx editconf -f $inp -o $out -c -d 1 -bt cubic inp=$out out=\"${proot}_${ff}_${wat}_solv\" gmx solvate -p $top -cp $inp -o $out The first command edits the box so that the molecule is centered ( -c ) in the cubic ( -bt cubic ) simulation box and it has 1 nm between molecule and box sides ( -d 1 ). The second command simply solvates the molecule filling the box with the water model geometry of choice.","title":"Editing the box and solvation"},{"location":"Gromacs/md_setup.html#energy-minimization","text":"onsidering that we are starting from a peptide conformation and a fixed water box configuration that are not particularly meaningful, we will first energy minimize the system using steepest descent. mdp=\"mini.mdp\" inp=\"${proot}_${ff}_${wat}_solv\" out=\"${proot}_${ff}_${wat}_mini\" gmx grompp -p $top -c $inp -f $mdp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out As we will do many times later, we first use the Gromacs preprocessor ( grompp ) and then run the calculation ( mdrun ). Since we are using the verbose ( -v ) option you will find that this produces a lot of output, corresponding to the gradual decrease in the energy of the system.","title":"Energy minimization"},{"location":"Gromacs/md_setup.html#md-in-the-canonical-ensemble-with-position-restraints","text":"We then equilibrate the water molecules while keeping the peptide mostly fixed using position restraints on its atoms. This is not too important in the context of a small peptide like the one we are considering, but can be critical for larger systems. mdp=\"nvt_posre.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt_posre\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"MD in the canonical ensemble with position restraints"},{"location":"Gromacs/md_setup.html#md-in-the-isothermal-isobaric-ensemble","text":"The next step is running MD and fixing the pressure to the value of interest (typically 1 bar) with the help of a barostat. Most of the remaining parameters stay the same as were in the NVT simulation. mdp=\"npt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_npt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"MD in the isothermal-isobaric ensemble"},{"location":"Gromacs/md_setup.html#production-run","text":"Finally we run a production run, again in the canonical ensemble, but now removing restraints on the peptide so that we can adequately sample conformational space. mdp=\"sd_nvt.mdp\" inp=$out out=\"${proot}_${ff}_${wat}_nvt\" gmx grompp -f $mdp -c $inp -p $top -r $inp -o $out -t ${inp}.cpt inp=$out out=$inp gmx mdrun -v -s $inp -deffnm $out","title":"Production run"},{"location":"Gromacs/submission.html","text":"Submission scripts for our local supercomputers We are using both the Arina and Atlas clusters for running jobs using the Gromacs software package. Below we show PBS scripts for both supercomputers. KALK #!/bin/bash #PBS -l nodes=nd15:ppn=16 #PBS -l walltime=480:00:00 #PBS -joe #PBS -q gpu2 ## Do not change below this line . /home/users/etc/send_funcs.sh HOST=$(hostname) echo Job runing on $HOST echo $PBS_O_WORKDIR > /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER chmod 755 /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER cd $PBS_O_WORKDIR chooseScr cd $scr #echo \"cp -r $PBS_O_WORKDIR/* $scr/.\" cp -r $PBS_O_WORKDIR/*tpr $scr/. #source /software/gromacs/bin/GMXRC.bash #module load GROMACS/2019.4-fosscuda-2018b module load GROMACS/2020-fosscuda-2018b tpr=\"topol.tpr\" /usr/bin/time -p mpirun -np 1 gmx_mpi mdrun -ntomp 16 -quiet -nb gpu -gpu_id 01 -s $tpr > $PBS_JOBID.out recover rm -f /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER ATLAS-FDR #!/bin/bash #PBS -q qchem #PBS -l nodes=1:ppn=16 #PBS -l mem=1gb #PBS -l cput=1000:00:00 #PBS -N qchem module load GROMACS/2019.4-foss-2019b cd $PBS_O_WORKDIR export NPROCS=`wc -l < $PBS_NODEFILE` tpr=\"topol.tpr\" np=8 ntomp=2 gmx mdrun -ntmpi $np -ntomp $ntomp -s $tpr ATLAS-EDR We are also using the extension of the Atlas supercomputer, which uses SLURM instead of PBS. A submission script for Gromacs looks as follows. You can check the documentation for options in Slurm here . #!/bin/bash #SBATCH --partition=long #SBATCH --job-name=hase #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=1 #SBATCH --gres=gpu:p40:1 #SBATCH --time=1-00:00:00 #SBATCH --mem=64G module load GROMACS/2020-fosscuda-2019b tpr=\"topol.tpr\" np=8 ntomp=1 ngpu=2 mpirun --map-by ppr:4:node -np $np gmx_mpi mdrun -ntomp $ntomp -s $tpr","title":"Submission scripts"},{"location":"Gromacs/submission.html#submission-scripts-for-our-local-supercomputers","text":"We are using both the Arina and Atlas clusters for running jobs using the Gromacs software package. Below we show PBS scripts for both supercomputers.","title":"Submission scripts for our local supercomputers"},{"location":"Gromacs/submission.html#kalk","text":"#!/bin/bash #PBS -l nodes=nd15:ppn=16 #PBS -l walltime=480:00:00 #PBS -joe #PBS -q gpu2 ## Do not change below this line . /home/users/etc/send_funcs.sh HOST=$(hostname) echo Job runing on $HOST echo $PBS_O_WORKDIR > /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER chmod 755 /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER cd $PBS_O_WORKDIR chooseScr cd $scr #echo \"cp -r $PBS_O_WORKDIR/* $scr/.\" cp -r $PBS_O_WORKDIR/*tpr $scr/. #source /software/gromacs/bin/GMXRC.bash #module load GROMACS/2019.4-fosscuda-2018b module load GROMACS/2020-fosscuda-2018b tpr=\"topol.tpr\" /usr/bin/time -p mpirun -np 1 gmx_mpi mdrun -ntomp 16 -quiet -nb gpu -gpu_id 01 -s $tpr > $PBS_JOBID.out recover rm -f /home/qsub_priv/pbs_paths/PBS.$PBS_JOBID.$USER","title":"KALK"},{"location":"Gromacs/submission.html#atlas-fdr","text":"#!/bin/bash #PBS -q qchem #PBS -l nodes=1:ppn=16 #PBS -l mem=1gb #PBS -l cput=1000:00:00 #PBS -N qchem module load GROMACS/2019.4-foss-2019b cd $PBS_O_WORKDIR export NPROCS=`wc -l < $PBS_NODEFILE` tpr=\"topol.tpr\" np=8 ntomp=2 gmx mdrun -ntmpi $np -ntomp $ntomp -s $tpr","title":"ATLAS-FDR"},{"location":"Gromacs/submission.html#atlas-edr","text":"We are also using the extension of the Atlas supercomputer, which uses SLURM instead of PBS. A submission script for Gromacs looks as follows. You can check the documentation for options in Slurm here . #!/bin/bash #SBATCH --partition=long #SBATCH --job-name=hase #SBATCH --nodes=2 #SBATCH --ntasks-per-node=4 #SBATCH --cpus-per-task=1 #SBATCH --gres=gpu:p40:1 #SBATCH --time=1-00:00:00 #SBATCH --mem=64G module load GROMACS/2020-fosscuda-2019b tpr=\"topol.tpr\" np=8 ntomp=1 ngpu=2 mpirun --map-by ppr:4:node -np $np gmx_mpi mdrun -ntomp $ntomp -s $tpr","title":"ATLAS-EDR"},{"location":"Linux/admin.html","text":"Linux Administration Creating and mounting new partitions In order to create a /scratch partition for my Linux server I had to first create a partition for a hard drive and then mount it in a new location. The first step needed for this was made easy by the fdisk program. Initially when running it I would get something like this username@machinename:$ sudo fdisk -l [sudo] password: Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 255 heads, 63 sectors/track, 15181 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0x00000000 Disk /dev/sdb doesn't contain a valid partition table Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux You can see that there are two drives, sda and sdb, the latter of which does not have a partition table. To create a new partition I simply typed sudo fdisk /dev/sdb and then I entered the dialogue from the fdisk program. First I created a new partition accepting most of the defaults, which resulted in the following output for fdisk -l Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 208 heads, 2 sectors/track, 586288 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0xc7a7c4cd Device Boot Start End Blocks Id System /dev/sdb1 256 243895807 975582208 83 Linux Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux Note the change in that now /dev/sdb1 appears. The next step was formating the partition, which can be achieved by typing in the command line sudo mkfs.ext4 /dev/sdb1 Then I created a mount point, which in this case is called /scratch and mounting the new partition sdb1 in that mount point. sudo mkdir /scratch sudo mount /dev/sdb1 /scratch The final bit was getting my system to mount that partition automatically, by editing the /etc/fstab file. For this I needed the UUID for my partition. This can be obtained from ls -l /dev/disk/by-uuid/ Then one must edit the /etc/fstab file with the UUID for sdb1, the mount point and the type of partition in a line looking very much like the one that follows UUID=31321321zxcgdfsdg-sdfgsdfaadsas31245 /scratch ext4 errors=remount-ro 0 2` A final detail was to recursively give read+write permissions to everyone. sudo chmod -R 777 /scratch/","title":"Linux administration"},{"location":"Linux/admin.html#linux-administration","text":"","title":"Linux Administration"},{"location":"Linux/admin.html#creating-and-mounting-new-partitions","text":"In order to create a /scratch partition for my Linux server I had to first create a partition for a hard drive and then mount it in a new location. The first step needed for this was made easy by the fdisk program. Initially when running it I would get something like this username@machinename:$ sudo fdisk -l [sudo] password: Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 255 heads, 63 sectors/track, 15181 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0x00000000 Disk /dev/sdb doesn't contain a valid partition table Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux You can see that there are two drives, sda and sdb, the latter of which does not have a partition table. To create a new partition I simply typed sudo fdisk /dev/sdb and then I entered the dialogue from the fdisk program. First I created a new partition accepting most of the defaults, which resulted in the following output for fdisk -l Note: sector size is 4096 (not 512) Disk /dev/sdb: 999.0 GB, 998997229568 bytes 208 heads, 2 sectors/track, 586288 cylinders, total 243895808 sectors Units = sectors of 1 * 4096 = 4096 bytes Sector size (logical/physical): 4096 bytes / 4096 bytes I/O size (minimum/optimal): 4096 bytes / 4096 bytes Disk identifier: 0xc7a7c4cd Device Boot Start End Blocks Id System /dev/sdb1 256 243895807 975582208 83 Linux Disk /dev/sda: 128.0 GB, 128035676160 bytes 255 heads, 63 sectors/track, 15566 cylinders, total 250069680 sectors Units = sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk identifier: 0x000979b9 Device Boot Start End Blocks Id System /dev/sda1 * 2048 19531775 9764864 83 Linux /dev/sda2 19533822 250068991 115267585 5 Extended /dev/sda5 19533824 28801023 4633600 82 Linux swap / Solaris /dev/sda6 28803072 250068991 110632960 83 Linux Note the change in that now /dev/sdb1 appears. The next step was formating the partition, which can be achieved by typing in the command line sudo mkfs.ext4 /dev/sdb1 Then I created a mount point, which in this case is called /scratch and mounting the new partition sdb1 in that mount point. sudo mkdir /scratch sudo mount /dev/sdb1 /scratch The final bit was getting my system to mount that partition automatically, by editing the /etc/fstab file. For this I needed the UUID for my partition. This can be obtained from ls -l /dev/disk/by-uuid/ Then one must edit the /etc/fstab file with the UUID for sdb1, the mount point and the type of partition in a line looking very much like the one that follows UUID=31321321zxcgdfsdg-sdfgsdfaadsas31245 /scratch ext4 errors=remount-ro 0 2` A final detail was to recursively give read+write permissions to everyone. sudo chmod -R 777 /scratch/","title":"Creating and mounting new partitions"},{"location":"MCPB.py/force-field-editing.html","text":"Including parameters to a force field Since we will be adding the newly obtained parameters to the Amber99SB*-ILDN force field, the first thing we must do is to retrieve the force field directory, which can be found here . The file to be downloaded is ff99sb-star-ildn.tgz . Once downloaded, to decompress the file, simply run: tar -xvf ff99sb-star-ildn.tgz to obtain a directory called ff99sb-star-ildn.ff . This force field directory should then be stored either in gromacs/share/gromacs/top/ or in the working directory. A personal advice would be to store it in a separate directory where it is clearly stated that the edited version of the force field is stored, e.g., gromacs-1ZE9/share/gromacs/top/ . Once the directory is placed in the desired location, you are ready to start adding parameters. Checking for equivalent residues One importart step, is to check wheter the newly parametrized residues are chemicaly aquivalent in any way. For example, when working with exact copies of the same protein in a dimer, at least two residues will be equivalent. In those cases, all parameters of equivalent residues should be averaged. To see some examples of dimer systems where averaged parameters are added, look at systems 5LFY, 2LI9 or 2MGT in the following OSF repository . The first step is to compare nonbonded parameters \u03c3 and \u03b5 of the newly parametrized residue and the residue present in the original force field. To do so, you should look at the ff99sb-star-ildn.ff/ffnonbonded.itp file and the 1ze9_dry.amb2gmx/1ze9_dry_GMX.top file. If the parameters are the same, which is usually the case, one can ommit creating new atomtypes for each atom of the residue. Otherwise, new atomtypes for each of the atoms should be created. The parameters to be added in the ff99sb-star-ildn.ff/ directory are the following: In the aminoacids.rtp file a new residue with a new name should be created for each residue that you have parametrized. In this file, charges of each atom are to be specified. A new atomtype should be created for the atom(s) bonded to the metal. For example, in the case of His13, this being a HID residue and knowing that Zn(II) is coordinated via the NE2 atom, the new residue with the new atomtype should be added as: [ HID ] [ atoms ] ... NE2 NB -0.57270 ... would be converted to [ HDB ] [ atoms ] ... NE2 NB3 -0.22712 ... The metal ion should be added following the same steps. In the aminoacids.hdb and aminoacids.vsd files, you should copy and paste the same lines present for each residue and edit the residue name to match the newly created one. In the ffnonbonded.itp file, nonbonded parameters \u03c3 and \u03b5 must be added for every new atomtype created. In the ffbonded.itp file, bonded parameters for bonds , angles and dihedrals should be added. These paramters are to be found in the 1ze9_dry.amb2gmx/1ze9_dry_GMX.top file. The parameters to be added should include all parameters where the metal ion is present as well as the new atomtypes. That means that, for the previous example, all bonds , angles and dihedrals in which NE2 participates should also be redefined for the new atomtype (NB3). The next files are to be found in the gromacs-1ZE9/share/gromacs/top/ directory: In the residuetypes.dat file a new line for each new residue should be added, specifying they are protein . The same should be done for the metal ion: ... HID Protein HDB Protein . . . ZNB Protein ... In the specbond.dat file a line specifying the bonds between protein and metal ion should be added: ... HDB NE2 1 ZNB ZNB 4 0.2 HDB ZNB ... First column specifies the first residue taking part in the bond, the second colums specifies the first atom that forms the bonds, the third column specifies que ammount of special bonds this atom forms. The Fourth, fifth and sixth columns specify the same information for the second residue and atom forming the bond. The seventh column specifies the distance \u00b1 10% of the reference distance under which, a bond will be formed for the specified atoms. The last two colums once again define the residues forming the bond. Lastly, to start running simulations in Gromacs, you must edit the PDB file so that the newly parametrized residues have the new residue names, e.g., HIS13 should be changed to HDB. Once the editing is finished, you must indicate which is the force field you want to use. This can be done in two different ways: Define the $PATH in which the force field directory is stored and execute Gromacs from that directory: gmx=/usr/local/gromacs/gromacs-1ZE9/bin/gmx Store force field in the working directory, then when running gmx pdb2gmx you should select the force field present in the working directory. Lastly, when running gmx pdb2gmx , the following line should be used: gmx pdb2gmx -f *pdb -ignh -o ${out} -p ${out} -merge all","title":"Editing force fields"},{"location":"MCPB.py/force-field-editing.html#including-parameters-to-a-force-field","text":"Since we will be adding the newly obtained parameters to the Amber99SB*-ILDN force field, the first thing we must do is to retrieve the force field directory, which can be found here . The file to be downloaded is ff99sb-star-ildn.tgz . Once downloaded, to decompress the file, simply run: tar -xvf ff99sb-star-ildn.tgz to obtain a directory called ff99sb-star-ildn.ff . This force field directory should then be stored either in gromacs/share/gromacs/top/ or in the working directory. A personal advice would be to store it in a separate directory where it is clearly stated that the edited version of the force field is stored, e.g., gromacs-1ZE9/share/gromacs/top/ . Once the directory is placed in the desired location, you are ready to start adding parameters.","title":"Including parameters to a force field"},{"location":"MCPB.py/force-field-editing.html#checking-for-equivalent-residues","text":"One importart step, is to check wheter the newly parametrized residues are chemicaly aquivalent in any way. For example, when working with exact copies of the same protein in a dimer, at least two residues will be equivalent. In those cases, all parameters of equivalent residues should be averaged. To see some examples of dimer systems where averaged parameters are added, look at systems 5LFY, 2LI9 or 2MGT in the following OSF repository . The first step is to compare nonbonded parameters \u03c3 and \u03b5 of the newly parametrized residue and the residue present in the original force field. To do so, you should look at the ff99sb-star-ildn.ff/ffnonbonded.itp file and the 1ze9_dry.amb2gmx/1ze9_dry_GMX.top file. If the parameters are the same, which is usually the case, one can ommit creating new atomtypes for each atom of the residue. Otherwise, new atomtypes for each of the atoms should be created. The parameters to be added in the ff99sb-star-ildn.ff/ directory are the following: In the aminoacids.rtp file a new residue with a new name should be created for each residue that you have parametrized. In this file, charges of each atom are to be specified. A new atomtype should be created for the atom(s) bonded to the metal. For example, in the case of His13, this being a HID residue and knowing that Zn(II) is coordinated via the NE2 atom, the new residue with the new atomtype should be added as: [ HID ] [ atoms ] ... NE2 NB -0.57270 ... would be converted to [ HDB ] [ atoms ] ... NE2 NB3 -0.22712 ... The metal ion should be added following the same steps. In the aminoacids.hdb and aminoacids.vsd files, you should copy and paste the same lines present for each residue and edit the residue name to match the newly created one. In the ffnonbonded.itp file, nonbonded parameters \u03c3 and \u03b5 must be added for every new atomtype created. In the ffbonded.itp file, bonded parameters for bonds , angles and dihedrals should be added. These paramters are to be found in the 1ze9_dry.amb2gmx/1ze9_dry_GMX.top file. The parameters to be added should include all parameters where the metal ion is present as well as the new atomtypes. That means that, for the previous example, all bonds , angles and dihedrals in which NE2 participates should also be redefined for the new atomtype (NB3). The next files are to be found in the gromacs-1ZE9/share/gromacs/top/ directory: In the residuetypes.dat file a new line for each new residue should be added, specifying they are protein . The same should be done for the metal ion: ... HID Protein HDB Protein . . . ZNB Protein ... In the specbond.dat file a line specifying the bonds between protein and metal ion should be added: ... HDB NE2 1 ZNB ZNB 4 0.2 HDB ZNB ... First column specifies the first residue taking part in the bond, the second colums specifies the first atom that forms the bonds, the third column specifies que ammount of special bonds this atom forms. The Fourth, fifth and sixth columns specify the same information for the second residue and atom forming the bond. The seventh column specifies the distance \u00b1 10% of the reference distance under which, a bond will be formed for the specified atoms. The last two colums once again define the residues forming the bond. Lastly, to start running simulations in Gromacs, you must edit the PDB file so that the newly parametrized residues have the new residue names, e.g., HIS13 should be changed to HDB. Once the editing is finished, you must indicate which is the force field you want to use. This can be done in two different ways: Define the $PATH in which the force field directory is stored and execute Gromacs from that directory: gmx=/usr/local/gromacs/gromacs-1ZE9/bin/gmx Store force field in the working directory, then when running gmx pdb2gmx you should select the force field present in the working directory. Lastly, when running gmx pdb2gmx , the following line should be used: gmx pdb2gmx -f *pdb -ignh -o ${out} -p ${out} -merge all","title":"Checking for equivalent residues"},{"location":"MCPB.py/installation.html","text":"Installing MCPB.py Since MCPB.py is a program included in AmberTools15 and onward versions, the first step is to install AmberTools. AmberTools can be easily installed as a conda environment: conda create --name AmberTools21 conda activate AmberTools21 conda install -c conda-forge ambertools=21 compilers conda update -c conda-forge ambertools Once AmberTools is installed, the MCPB.py program will be included by default.","title":"Installation"},{"location":"MCPB.py/installation.html#installing-mcpbpy","text":"Since MCPB.py is a program included in AmberTools15 and onward versions, the first step is to install AmberTools. AmberTools can be easily installed as a conda environment: conda create --name AmberTools21 conda activate AmberTools21 conda install -c conda-forge ambertools=21 compilers conda update -c conda-forge ambertools Once AmberTools is installed, the MCPB.py program will be included by default.","title":"Installing MCPB.py"},{"location":"MCPB.py/parametrization.html","text":"Parametrization setup These brief instructions are a general guide to how to parametrize a simple metalloprotein system. More detailed tutorials for a variety of systems can be found elsewhere . In this case we are simulating a monomeric Amyloid \u03b2 - Zn(II) system with a 1:1 stoichiometry using the AmberTools21 software version, but things should not change too much for other recent versions of the software. The parameters will be obtained for the Amber99SB*-ILDN force field. You must have a working version installed in your machine. Get the files In order to start with the parametrization, you must download the PDB file of the system. In this case, we will be working with the PDB ID: 1ZE9 system . The file must be downloaded in the PDB format. Preparing the files The first step is separating the different fragments present in the PDB file. One file must include the metal exclusively and another file should contain the protein chain. An additional file should be created if a ligand is present although this is not the case. The metal must be separated to a file. The name of the file must be in capital letters, i.e., in this case, since the metal being separated is Zn(II), the file must be named ZN.pdb. Then, the file must be converted from PDB to MOL2 format: metalpdb2mol2.py -i ZN.pdb -o ZN.mol2 -c 2 where, the charge must be specified with the -c flag. Then, the file containing the protein must be processed. Both the ACE and NH2 residues must be removed. Once the file is processed, protons must be added using Chimera. It is important to know the protonation state of each histidine (if present). In this case, both His6 and His14 are protonated in the \u03b5 N (HIE) and His13 is protonated in the \u03b4 N (HID). To protonate the protein: Open Chimera and load PDB file Tools > Structure Editing > AddH Consider H-bonds Protonation states for histidine > Residue-name-based Once the protein is protonated, both protein and metal PDB files must be merged: cat 1ze9_chimera.pdb ZN.pdb > 1ze9_H.pdb One must be cautious when merging PDB files, as the END line present in the protein PDB file will be written before the PDB of the metal. Therefore, removing the line is indispensable. Once the files are merged, the newly merged PDB file is renumbered: pdb4amber -i 1ze9_H.pdb -o 1ze9_renum_H.pdb Running the MCPB.py program The first thing needed is an input file for the MCPB.py program. This file (1ze9.in), should include the following information: original_pdb 1ze9_renum_H.pdb group_name 1ze9 cut_off 2.8 ion_ids 254 ion_mol2files ZN.mol2 large_opt 1 force_field ff99SB software_version g09 The cut_off value specifies that a bond exists between the metal ion and surrounding atoms, therefore, it must be double checked to ensure the cutoff value is selected correctly. To double check, the 1ze9_small_opt.com file should be visualized to ensure that the desired metal-protein bonds are present. The ion_ids value specifies the atom number of the ion present in the specified PDB file. The large_opt variable is used to indicate whether to do a geometry optimization in the Gaussian input file. With the value 1, we indicate that an optimization of the hydrogen positions will be done. More variables can be indicated in the input file, such as different force fields, different softwares to run QM calculations and many more. For more information see pages 288-290 in the Amber 2016 reference manual . Once the ipunt file is ready, the MPCB.py program is run: MCPB.py -i 1ze9.in -s 1 This line will create three Gaussian input files: 1. 1ze9_small_opt.com 2. 1ze9_small_fc.com 3. 1ze9_large_mk.com Each of the QM calculations must be done in the same order they are listed in. The first calculation will optimize the structure to give equilibrium geometry constants. Once the first calculation is done, the checkpoint file (.chk.gz) must be present in the directory in which the second calculation will be made. The second calculation will obtain force constants. In order to perform the third calculation, some processing of the second calculation checkpoint must be made: gzip -d 1ze9_small_opt.chk.gz formchk 1ze9_small_opt.chk > 1ze9_small_opt.fchk The processed checkpoint file must also be present in the directory where the third calculation will be performed. The third calculation performs the Merz-Kollman RESP charge calculation to obtain the optimized charges of the system. Next, the Seminario method will be used to generate force field parameters: MCPB.py -i 1ze9.in -s 2 This line will create the 1ze9_mcpbpy.frcmod file which will be used in the leap modelling. MCPB.py -i 1ze9.in -s 3 The RESP charge fitting is performed and the mol2 files for the metal site residues are obtained: ZN1.mol2 HE1.mol2 GU1.mol2 HD1.mol2 HE2.mol2 MCPB.py -i 1ze9.in -s 4 The tleap input file is generated ( 1ze9_tleap.in ) and a new PDB file is created, where the residues coordinating the metal ion are renamed ( 1ze9_mcpbpy.pdb ). In the new PDB file, ASP residues are to be renamed as ASH for tleap to process them. Occasionally, an incorrect bond could be created in the tleap input file. Bonds are defined as: bond mol.X.atm2 mol.Y.atm2 The X and Y define the residue number, whereas atm1 and atm2 define the atom name present in each residue. Each line will define a bond in such way. Generally, bonds between metal-protein as well as \"links\" between renamed residues and the rest of the protein chain should be present. Once the files are corrected, tleap is used to generate the topology and coordinate files: tleap -s -f 1ze9_tleap.in > 1ze9_tleap.out Tleap should have created a topology ( 1ze9_dry.prmtop ) and a coordinate file ( 1ze9_dry.inpcrd ). Lastly, parameters are converted from Amber to Gromacs synthax using acpype: acpype -p 1ze9_dry.prmtop -x 1ze9_dry.inpcrd -o gmx -n -1 -l A new directory called 1ze9_dry.amb2gmx is created where all Gromacs parameters are stored.","title":"Parametrization"},{"location":"MCPB.py/parametrization.html#parametrization-setup","text":"These brief instructions are a general guide to how to parametrize a simple metalloprotein system. More detailed tutorials for a variety of systems can be found elsewhere . In this case we are simulating a monomeric Amyloid \u03b2 - Zn(II) system with a 1:1 stoichiometry using the AmberTools21 software version, but things should not change too much for other recent versions of the software. The parameters will be obtained for the Amber99SB*-ILDN force field. You must have a working version installed in your machine.","title":"Parametrization setup"},{"location":"MCPB.py/parametrization.html#get-the-files","text":"In order to start with the parametrization, you must download the PDB file of the system. In this case, we will be working with the PDB ID: 1ZE9 system . The file must be downloaded in the PDB format.","title":"Get the files"},{"location":"MCPB.py/parametrization.html#preparing-the-files","text":"The first step is separating the different fragments present in the PDB file. One file must include the metal exclusively and another file should contain the protein chain. An additional file should be created if a ligand is present although this is not the case. The metal must be separated to a file. The name of the file must be in capital letters, i.e., in this case, since the metal being separated is Zn(II), the file must be named ZN.pdb. Then, the file must be converted from PDB to MOL2 format: metalpdb2mol2.py -i ZN.pdb -o ZN.mol2 -c 2 where, the charge must be specified with the -c flag. Then, the file containing the protein must be processed. Both the ACE and NH2 residues must be removed. Once the file is processed, protons must be added using Chimera. It is important to know the protonation state of each histidine (if present). In this case, both His6 and His14 are protonated in the \u03b5 N (HIE) and His13 is protonated in the \u03b4 N (HID). To protonate the protein: Open Chimera and load PDB file Tools > Structure Editing > AddH Consider H-bonds Protonation states for histidine > Residue-name-based Once the protein is protonated, both protein and metal PDB files must be merged: cat 1ze9_chimera.pdb ZN.pdb > 1ze9_H.pdb One must be cautious when merging PDB files, as the END line present in the protein PDB file will be written before the PDB of the metal. Therefore, removing the line is indispensable. Once the files are merged, the newly merged PDB file is renumbered: pdb4amber -i 1ze9_H.pdb -o 1ze9_renum_H.pdb","title":"Preparing the files"},{"location":"MCPB.py/parametrization.html#running-the-mcpbpy-program","text":"The first thing needed is an input file for the MCPB.py program. This file (1ze9.in), should include the following information: original_pdb 1ze9_renum_H.pdb group_name 1ze9 cut_off 2.8 ion_ids 254 ion_mol2files ZN.mol2 large_opt 1 force_field ff99SB software_version g09 The cut_off value specifies that a bond exists between the metal ion and surrounding atoms, therefore, it must be double checked to ensure the cutoff value is selected correctly. To double check, the 1ze9_small_opt.com file should be visualized to ensure that the desired metal-protein bonds are present. The ion_ids value specifies the atom number of the ion present in the specified PDB file. The large_opt variable is used to indicate whether to do a geometry optimization in the Gaussian input file. With the value 1, we indicate that an optimization of the hydrogen positions will be done. More variables can be indicated in the input file, such as different force fields, different softwares to run QM calculations and many more. For more information see pages 288-290 in the Amber 2016 reference manual . Once the ipunt file is ready, the MPCB.py program is run: MCPB.py -i 1ze9.in -s 1 This line will create three Gaussian input files: 1. 1ze9_small_opt.com 2. 1ze9_small_fc.com 3. 1ze9_large_mk.com Each of the QM calculations must be done in the same order they are listed in. The first calculation will optimize the structure to give equilibrium geometry constants. Once the first calculation is done, the checkpoint file (.chk.gz) must be present in the directory in which the second calculation will be made. The second calculation will obtain force constants. In order to perform the third calculation, some processing of the second calculation checkpoint must be made: gzip -d 1ze9_small_opt.chk.gz formchk 1ze9_small_opt.chk > 1ze9_small_opt.fchk The processed checkpoint file must also be present in the directory where the third calculation will be performed. The third calculation performs the Merz-Kollman RESP charge calculation to obtain the optimized charges of the system. Next, the Seminario method will be used to generate force field parameters: MCPB.py -i 1ze9.in -s 2 This line will create the 1ze9_mcpbpy.frcmod file which will be used in the leap modelling. MCPB.py -i 1ze9.in -s 3 The RESP charge fitting is performed and the mol2 files for the metal site residues are obtained: ZN1.mol2 HE1.mol2 GU1.mol2 HD1.mol2 HE2.mol2 MCPB.py -i 1ze9.in -s 4 The tleap input file is generated ( 1ze9_tleap.in ) and a new PDB file is created, where the residues coordinating the metal ion are renamed ( 1ze9_mcpbpy.pdb ). In the new PDB file, ASP residues are to be renamed as ASH for tleap to process them. Occasionally, an incorrect bond could be created in the tleap input file. Bonds are defined as: bond mol.X.atm2 mol.Y.atm2 The X and Y define the residue number, whereas atm1 and atm2 define the atom name present in each residue. Each line will define a bond in such way. Generally, bonds between metal-protein as well as \"links\" between renamed residues and the rest of the protein chain should be present. Once the files are corrected, tleap is used to generate the topology and coordinate files: tleap -s -f 1ze9_tleap.in > 1ze9_tleap.out Tleap should have created a topology ( 1ze9_dry.prmtop ) and a coordinate file ( 1ze9_dry.inpcrd ). Lastly, parameters are converted from Amber to Gromacs synthax using acpype: acpype -p 1ze9_dry.prmtop -x 1ze9_dry.inpcrd -o gmx -n -1 -l A new directory called 1ze9_dry.amb2gmx is created where all Gromacs parameters are stored.","title":"Running the MCPB.py program"},{"location":"MET-parametrize/parametrization.html","text":"Parametrizing an amino acid using AmberTools21 Below is a step-by-step guide for amino acid parametrization using the tools provided with the Amber MD package . Specifically, we will be using Ambertools , which can be installed using Anaconda simply typing conda install -c conda-forge ambertools compilers Within the Amber Build a structure In order to parametrize an amino acid, the first thing we will need is a structure, which we will normally build in the acetylated and amidated form (i.e. with ACE and NME caps). Here we illustrata parametrization with the methionine residue, and hence we will need a structure for ACE-MET-NME (we can generate it using tleap or Gaussview ). In this example, we have created the structure for you. It can be downloaded from the following link . Mind that atom numbers are ordered to match the sequence (ACE-MET-NME), and that in the case of the aminoacid, the atom number of the atoms of MET match the order in which they appear in GROMACS . The order of the atoms will be important for future steps. Choose your force field Once the structure is built, one must follow the steps mentioned in the relevant article of the force field of choice. Here, we will generate parameters for the Amber99SB*-ILDN force field, which includes corrections in both the backbone and sidechains torsions. Quantum-mechanical calculations In order to generate charges for the amino acid residue, we must run QM calculations, which was done in the Amber ff94 paper using MP2/6-31G*. Here, we will use B3LYP/6-31G*, which should not affect the results much. To perform the optimization of the structure, we will be using the Gaussian16 software that is available both in the Arina and Atlas clusters. The input file for geometry optimization can be found here . The file looks something like this %chk=00.chk # B3LYP/6-31G* Opt Methionine 0 1 C 2.000000 2.090000 0.000000 H 1.486000 2.454000 0.890000 H 1.486000 2.454000 -0.890000 ... where we have ommited most of the coordinates in the input. For convenience, we have prepared a folder including all the output files . Once the geometry optimization is done, a second calculation must be performed to derive the charges. For this, you will need the following input file . The contents of the file are as follows: %chk=00.chk # B3LYP/6-31G* geom=check guess=read Integral=(Grid=UltraFine) Pop(MK,ReadRadii) IOp(6/33=2,6/42=6) Methionine 0 1 With the Integral keyword we modify the method of computation and use two-electron integrals and their derivatives. With the Population keyword we print molecular orbitals and several types of population analysis and atomic charge assignements. Lastly, with the IOp keyword we set internal options. This calculation must be run in the same directory as the 00.chk.gz file as the geometry will be read from it. All the configurations for this last part have been taken from the workflow created with the MCPB.py program of AmberTools . The output from this calculation can be downloaded from here . Charge fitting Next we jump into the actual parametrization of the molecular mechanics force field. All the files necessary for this part are available in this folder . The first step is to perform the fitting of the charges using the Restrained Electrostatic Potential (RESP) method. The electrostatic potential has to be reformated to obtain a RESP-friendly syntax. We will achieve so by processing the Gaussian output with the espgen program: espgen -i MET-charges.log -o esp.dat Then, two additional files must be created: resp.in and resp.qin . The resp.in file indicates a variety of options for the RESP calculation. It looks as follows: capped-resp run 1 #This is name of the calculation, it is up to you &cntrl nmol=1, # Number of molecules included in the calculation ihfree=1, # Weak restraints only in the heavy atoms qwt=0.0005, # This is the strength of the restraint in A.U iqopt=2, # Read the initial charges from the resp.qin file / 1 # Number of the molecule included MET # Name of the residue, this info is just for you 0 29 # Charge of the molecule and number of atoms ... This file header is followed by two additional columns of numbers. The first column indicates the atomic number of each atom. The value in the second column determines whether the charge must be calculated (if the value is 0) or, alternatively, the charge is defined by the user (if the value -1). In the latter case, the charge will be set in the resp.qin file. In the current parametrization of a Met residue, the lines corresponding to ACE and NME atoms have a value of -1; hence their charges are set by the user. We can find them in the resp.qin file. The charges of the NH- and CO- are also indicated, as for the Amber99SB*-ILDN force field these charges are the same for all amino acids. Finally, if a number other than 0 or -1 appears in the second column for a given atom, it will be constrained to have the same charge as that corresponding to the atom index (e.g., H12 and H13 or H15 and H16 in our example). Once these files are ready, we run the RESP calculation: resp -O -i resp.in -o resp.out -p resp.pch -t resp.chg -q resp.qin -e esp.dat If you get error messages such as At line 403 of file resp.F (unit=5,file='resp.in') error, you have to look at the spaces present in your resp.in file. Mind that a blank line must be added at the end of the resp.in file. The next step in our workflow is to run the antechamber program, which will associate the partial charges to each atom: antechamber -fi gout -i MET-charges.log -bk MET -fo ac -o MET.ac -c rc -cf resp.chg -at amber Now, we will delete the capping groups from the custom residue. To do so we need the MET.mc file, which is pretty self explanatory. The omited atoms are the ones belonging to the caps. One must then run the following line: prepgen -i MET.ac -o MET.prepin -m MET.mc -rn MET Lastly, we will generate the parameters using parmchk2 : parmchk2 -i MET.ac -f ac -o MET_gaff.frcmod The final step is then to run tleap to obtain the Amber parameters. Open tleap by typing tleap in your terminal. Once inside, type the following lines. loadamberprep MET.prepin source oldff/leaprc.ff99 loadamberparams MET_gaff.frcmod aa = sequence {ACE MET NME} saveAmberParm aa MET.prmtop MET.inpcrd quit To convert the parameters from Amber to GROMACS we then run acpype: acpype -p MET.prmtop -x MET.inpcrd -o gmx -n -1 -l Parameters with the GROMACS syntax will be located to a directory called MET.amb2gmx . GROMACS parameters are available in this folder: GROMACS-files .","title":"Aminoacid residue"},{"location":"MET-parametrize/parametrization.html#parametrizing-an-amino-acid-using-ambertools21","text":"Below is a step-by-step guide for amino acid parametrization using the tools provided with the Amber MD package . Specifically, we will be using Ambertools , which can be installed using Anaconda simply typing conda install -c conda-forge ambertools compilers Within the Amber","title":"Parametrizing an amino acid using AmberTools21"},{"location":"MET-parametrize/parametrization.html#build-a-structure","text":"In order to parametrize an amino acid, the first thing we will need is a structure, which we will normally build in the acetylated and amidated form (i.e. with ACE and NME caps). Here we illustrata parametrization with the methionine residue, and hence we will need a structure for ACE-MET-NME (we can generate it using tleap or Gaussview ). In this example, we have created the structure for you. It can be downloaded from the following link . Mind that atom numbers are ordered to match the sequence (ACE-MET-NME), and that in the case of the aminoacid, the atom number of the atoms of MET match the order in which they appear in GROMACS . The order of the atoms will be important for future steps.","title":"Build a structure"},{"location":"MET-parametrize/parametrization.html#choose-your-force-field","text":"Once the structure is built, one must follow the steps mentioned in the relevant article of the force field of choice. Here, we will generate parameters for the Amber99SB*-ILDN force field, which includes corrections in both the backbone and sidechains torsions.","title":"Choose your force field"},{"location":"MET-parametrize/parametrization.html#quantum-mechanical-calculations","text":"In order to generate charges for the amino acid residue, we must run QM calculations, which was done in the Amber ff94 paper using MP2/6-31G*. Here, we will use B3LYP/6-31G*, which should not affect the results much. To perform the optimization of the structure, we will be using the Gaussian16 software that is available both in the Arina and Atlas clusters. The input file for geometry optimization can be found here . The file looks something like this %chk=00.chk # B3LYP/6-31G* Opt Methionine 0 1 C 2.000000 2.090000 0.000000 H 1.486000 2.454000 0.890000 H 1.486000 2.454000 -0.890000 ... where we have ommited most of the coordinates in the input. For convenience, we have prepared a folder including all the output files . Once the geometry optimization is done, a second calculation must be performed to derive the charges. For this, you will need the following input file . The contents of the file are as follows: %chk=00.chk # B3LYP/6-31G* geom=check guess=read Integral=(Grid=UltraFine) Pop(MK,ReadRadii) IOp(6/33=2,6/42=6) Methionine 0 1 With the Integral keyword we modify the method of computation and use two-electron integrals and their derivatives. With the Population keyword we print molecular orbitals and several types of population analysis and atomic charge assignements. Lastly, with the IOp keyword we set internal options. This calculation must be run in the same directory as the 00.chk.gz file as the geometry will be read from it. All the configurations for this last part have been taken from the workflow created with the MCPB.py program of AmberTools . The output from this calculation can be downloaded from here .","title":"Quantum-mechanical calculations"},{"location":"MET-parametrize/parametrization.html#charge-fitting","text":"Next we jump into the actual parametrization of the molecular mechanics force field. All the files necessary for this part are available in this folder . The first step is to perform the fitting of the charges using the Restrained Electrostatic Potential (RESP) method. The electrostatic potential has to be reformated to obtain a RESP-friendly syntax. We will achieve so by processing the Gaussian output with the espgen program: espgen -i MET-charges.log -o esp.dat Then, two additional files must be created: resp.in and resp.qin . The resp.in file indicates a variety of options for the RESP calculation. It looks as follows: capped-resp run 1 #This is name of the calculation, it is up to you &cntrl nmol=1, # Number of molecules included in the calculation ihfree=1, # Weak restraints only in the heavy atoms qwt=0.0005, # This is the strength of the restraint in A.U iqopt=2, # Read the initial charges from the resp.qin file / 1 # Number of the molecule included MET # Name of the residue, this info is just for you 0 29 # Charge of the molecule and number of atoms ... This file header is followed by two additional columns of numbers. The first column indicates the atomic number of each atom. The value in the second column determines whether the charge must be calculated (if the value is 0) or, alternatively, the charge is defined by the user (if the value -1). In the latter case, the charge will be set in the resp.qin file. In the current parametrization of a Met residue, the lines corresponding to ACE and NME atoms have a value of -1; hence their charges are set by the user. We can find them in the resp.qin file. The charges of the NH- and CO- are also indicated, as for the Amber99SB*-ILDN force field these charges are the same for all amino acids. Finally, if a number other than 0 or -1 appears in the second column for a given atom, it will be constrained to have the same charge as that corresponding to the atom index (e.g., H12 and H13 or H15 and H16 in our example). Once these files are ready, we run the RESP calculation: resp -O -i resp.in -o resp.out -p resp.pch -t resp.chg -q resp.qin -e esp.dat If you get error messages such as At line 403 of file resp.F (unit=5,file='resp.in') error, you have to look at the spaces present in your resp.in file. Mind that a blank line must be added at the end of the resp.in file. The next step in our workflow is to run the antechamber program, which will associate the partial charges to each atom: antechamber -fi gout -i MET-charges.log -bk MET -fo ac -o MET.ac -c rc -cf resp.chg -at amber Now, we will delete the capping groups from the custom residue. To do so we need the MET.mc file, which is pretty self explanatory. The omited atoms are the ones belonging to the caps. One must then run the following line: prepgen -i MET.ac -o MET.prepin -m MET.mc -rn MET Lastly, we will generate the parameters using parmchk2 : parmchk2 -i MET.ac -f ac -o MET_gaff.frcmod The final step is then to run tleap to obtain the Amber parameters. Open tleap by typing tleap in your terminal. Once inside, type the following lines. loadamberprep MET.prepin source oldff/leaprc.ff99 loadamberparams MET_gaff.frcmod aa = sequence {ACE MET NME} saveAmberParm aa MET.prmtop MET.inpcrd quit To convert the parameters from Amber to GROMACS we then run acpype: acpype -p MET.prmtop -x MET.inpcrd -o gmx -n -1 -l Parameters with the GROMACS syntax will be located to a directory called MET.amb2gmx . GROMACS parameters are available in this folder: GROMACS-files .","title":"Charge fitting"},{"location":"MacOSX/hacks.html","text":"Mac OS X hacks These are just a few tricks that made my life incredibly easier when transitioning from a Ubuntu Linux machine to working on a Mac desktop computer. Mounting ext2/ext3 volumes This was useful for getting my backups to be read-write-able in my Mac. This is taken from the notes that Uditha Atukorala wrote for the WireFrame 1. Install MacFUSE If you haven not already installed it download and install MacFUSE . 2. Install FUSE - Ext2 Once you have MacFUSE download and install fuse-ext2 . Even though it says fuse-ext2, this one package gives both ext2 and ext3 read-write support. After installation you should see both MacFUSE and fuse-ext2 icons in System Preferences. You now have support for ext2 and ext3 file systems. When you plug in an external ext2/ext3 partition it should automatically show up in Finder, mounted and ready to use. If auto-mount is not giving you read/write access to ext2/ext3 partitions then you will have to edit the auto-mount script for fuse-ext2 which can be found at /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util . $ sudo vi -c /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util Around line 207 (in function Mount () ) you will find the line OPTIONS=\"auto_xattr,defer_permissions\" . Change that line to read as OPTIONS=\"auto_xattr,defer_permissions,rw+\" . ... function Mount () { LogDebug \"[Mount] Entering function Mount...\" # Setting both defer_auth and defer_permissions. The option was renamed # starting with MacFUSE 1.0.0, and there seems to be no backward # compatibility on the options. # OPTIONS=\"auto_xattr,defer_permissions\" OPTIONS=\"auto_xattr,defer_permissions,rw+\" # The local option is only enabled on Leopard. It causes strange ... } This last bit was what actually solved my problem.","title":"Mac OS X hacks"},{"location":"MacOSX/hacks.html#mac-os-x-hacks","text":"These are just a few tricks that made my life incredibly easier when transitioning from a Ubuntu Linux machine to working on a Mac desktop computer.","title":"Mac OS X hacks"},{"location":"MacOSX/hacks.html#mounting-ext2ext3-volumes","text":"This was useful for getting my backups to be read-write-able in my Mac. This is taken from the notes that Uditha Atukorala wrote for the WireFrame","title":"Mounting ext2/ext3 volumes"},{"location":"MacOSX/hacks.html#1-install-macfuse","text":"If you haven not already installed it download and install MacFUSE .","title":"1. Install MacFUSE"},{"location":"MacOSX/hacks.html#2-install-fuse-ext2","text":"Once you have MacFUSE download and install fuse-ext2 . Even though it says fuse-ext2, this one package gives both ext2 and ext3 read-write support. After installation you should see both MacFUSE and fuse-ext2 icons in System Preferences. You now have support for ext2 and ext3 file systems. When you plug in an external ext2/ext3 partition it should automatically show up in Finder, mounted and ready to use. If auto-mount is not giving you read/write access to ext2/ext3 partitions then you will have to edit the auto-mount script for fuse-ext2 which can be found at /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util . $ sudo vi -c /System/Library/Filesystems/fuse-ext2.fs/fuse-ext2.util Around line 207 (in function Mount () ) you will find the line OPTIONS=\"auto_xattr,defer_permissions\" . Change that line to read as OPTIONS=\"auto_xattr,defer_permissions,rw+\" . ... function Mount () { LogDebug \"[Mount] Entering function Mount...\" # Setting both defer_auth and defer_permissions. The option was renamed # starting with MacFUSE 1.0.0, and there seems to be no backward # compatibility on the options. # OPTIONS=\"auto_xattr,defer_permissions\" OPTIONS=\"auto_xattr,defer_permissions,rw+\" # The local option is only enabled on Leopard. It causes strange ... } This last bit was what actually solved my problem.","title":"2. Install FUSE - Ext2"},{"location":"Markdown/syntax.html","text":"Markdown Syntax The syntax that allows us to write this web site is Markdown . You can find more information about it on the internet but here you have some initial tips. Block Elements. Paragraphs and line breaks. For a new paragraph, use two enter keys. Double blank space is not supported. For a line break, you will need two space bar keys before using the enter key. Headers. Headers are created by levels using # symbol. Here you have an example: # Header 1 ## Header 2 ### Header 3 #### Header 4 will correspond to: Header 1 Header 2 Header 3 Header 4 Quotes. To quote text you must use the symbol > at the start of your text: > Hace falta toda una vida para aprender a vivir. - S\u00e9neca will give you: Hace falta toda una vida para aprender a vivir. - S\u00e9neca Lists. There are two types of lists: unordered using * , - or + and ordered using numbers. You can also nest elements of the lists by using four space bar keys before the corresponding symbol of your list. Here an example: - Unordered List + Element 1 You can include paragraphs or other text inside lists by leaving a blank space after and the corresponding _space bar_ keys for the nested element (8 in this case). + Element 2 + Element 3 * Ordered List 1. Ordered element 1 2. Ordered element 2 will give you: Unordered List Element 1 You can include paragraphs or other text inside lists by leaving a blank space after and the corresponding space bar keys for the nested element (8 in this case). + Element 2 + Element 3 * Ordered List 1. Ordered element 1 2. Ordered element 2 Code. For writing code you can use the ``` symbol before and after to create code blocks that will be visualized inside a code box. For \"inline code\" see Code in the Line Elements section (below). \\``` echo \"Hello World\" \\``` will give you: echo \"Hello World\" Horizontal lines. Horizontal lines are used to enhance the visualization and reading of your documentation. Notice how they have been used to separate the sections in this document. You can place them by repeating the symbol * , - or _ three times. It is always a good idea to separate them with spaces (see the example below) and leave blank spaces before and after. * * * - - - _ _ _ will give you: Line Elements. Emphasis. To emphasis text, markdown have options line bold text and italic . Use one * symbol before and after the text to apply italic and two for bold. You can apply both by using the symbol three times (combining bold and italic). Links. For introducing links in the text, use the syntax [text to hyperlink] followed by (URL or .md file path inside repo). Here you have an example where our BioKT Lab Page is hyperlinked using BioKT Lab Page . Code. If you want to introduce some commands \"in line\", like this , you have to use the ` symbol before and after. Others. Images. Skip Markdown syntax. If you want to ignore the characters of the Markdown syntax, use \\ before any character.","title":"Syntax"},{"location":"Markdown/syntax.html#markdown-syntax","text":"The syntax that allows us to write this web site is Markdown . You can find more information about it on the internet but here you have some initial tips.","title":"Markdown Syntax"},{"location":"Markdown/syntax.html#block-elements","text":"Paragraphs and line breaks. For a new paragraph, use two enter keys. Double blank space is not supported. For a line break, you will need two space bar keys before using the enter key. Headers. Headers are created by levels using # symbol. Here you have an example: # Header 1 ## Header 2 ### Header 3 #### Header 4 will correspond to:","title":"Block Elements."},{"location":"Markdown/syntax.html#header-1","text":"","title":"Header 1"},{"location":"Markdown/syntax.html#header-2","text":"","title":"Header 2"},{"location":"Markdown/syntax.html#header-3","text":"","title":"Header 3"},{"location":"Markdown/syntax.html#header-4","text":"Quotes. To quote text you must use the symbol > at the start of your text: > Hace falta toda una vida para aprender a vivir. - S\u00e9neca will give you: Hace falta toda una vida para aprender a vivir. - S\u00e9neca Lists. There are two types of lists: unordered using * , - or + and ordered using numbers. You can also nest elements of the lists by using four space bar keys before the corresponding symbol of your list. Here an example: - Unordered List + Element 1 You can include paragraphs or other text inside lists by leaving a blank space after and the corresponding _space bar_ keys for the nested element (8 in this case). + Element 2 + Element 3 * Ordered List 1. Ordered element 1 2. Ordered element 2 will give you: Unordered List Element 1 You can include paragraphs or other text inside lists by leaving a blank space after and the corresponding space bar keys for the nested element (8 in this case). + Element 2 + Element 3 * Ordered List 1. Ordered element 1 2. Ordered element 2 Code. For writing code you can use the ``` symbol before and after to create code blocks that will be visualized inside a code box. For \"inline code\" see Code in the Line Elements section (below). \\``` echo \"Hello World\" \\``` will give you: echo \"Hello World\" Horizontal lines. Horizontal lines are used to enhance the visualization and reading of your documentation. Notice how they have been used to separate the sections in this document. You can place them by repeating the symbol * , - or _ three times. It is always a good idea to separate them with spaces (see the example below) and leave blank spaces before and after. * * * - - - _ _ _ will give you:","title":"Header 4"},{"location":"Markdown/syntax.html#line-elements","text":"Emphasis. To emphasis text, markdown have options line bold text and italic . Use one * symbol before and after the text to apply italic and two for bold. You can apply both by using the symbol three times (combining bold and italic). Links. For introducing links in the text, use the syntax [text to hyperlink] followed by (URL or .md file path inside repo). Here you have an example where our BioKT Lab Page is hyperlinked using BioKT Lab Page . Code. If you want to introduce some commands \"in line\", like this , you have to use the ` symbol before and after.","title":"Line Elements."},{"location":"Markdown/syntax.html#others","text":"Images. Skip Markdown syntax. If you want to ignore the characters of the Markdown syntax, use \\ before any character.","title":"Others."},{"location":"MkDocs/installation.html","text":"Installing MkDocs and other software. The files that are used by MkDocs to built this website are located in the BioKT/ResDocs repository on GitHub BioKT (let's call it remote repository). Although public, it requires some permissions to contribute directly. Once this permissions are aqcuired you are ready to start. In order to modify/create files, all the work will be done locally in a \"clone\" of the original remote repository. Changes made will be then \"pushed\" to the remote repo and used to re-built the website. The softwares required for this work are Git, MkDocs and presumably GitHub CLI. - Git is the version control to update/modify/create the files. - MkDocs builts the website. - GitHub CLI allows you to connect your local repo with the remote one via your GitHub credentials (HTTPS protocol). Installing software. Git. Windows: Git could be already installed. If not, go to the Git website and download it. MacOS: Check if Git is already available: git --version . If not, it will prompt you to install it. Linux: Ubuntu: sudo apt install git-all should work. MkDocs. MkDocs requires a recent version of Python and the python package manager pip. MacOS: Simply run the following command: pip install mkdocs . Linux: Simply run the command : pip install mkdocs . Windows: Run: 'py -m pip install mkdocs' GitHub CLI. MacOS: Via Conda with the command: conda install gh --channel conda-forge . Once installed, in the command line enter gh auth login and follow the instructions. Select HTTPS as preferred protocol. Use GitHub credentials to authenticate to Git. Linux: Follow the instructions here Contribute process. Now you are ready to start your contributions! First thing you want to do is create a directory in your computer to store the clone of the remote repository: mkdir BioKT_toolbox cd BioKT_toolbox Inside this directory clone the remote repository with: git clone https://github.com/BioKT/ResDocs.git If this is your first time, the cloned repository will be up to date. However, whenever in the future you want to contribute, you will need to ensure that your local repository is updated with the last contributions. Otherwise some unexpected errors will occur when trying to push your contributions into the remote repo. In order to update your local repo it would be enough to run the command below. REMEMBER, YOU SHOULD RUN THIS COMMAND EVERY SINGLE TIME YOU START A NEW CONTRIBUTION . git pull origin master If this is your first time updating the local repo, you will probably get a message talking about how should Git behave when it has to reconcile changes from your local repo and the remote repo. Just copy the first line that it recommends, something like: git config pull.rebase false # merge (the default strategy) Now you can modify the corresponding files of any section you want or create your new section or subsections. Modifying already existing files is easy. Just use \"vi\" or whatever text editor you like and then save the changes. If you want to know how the syntax works (which is Markdown btw), check this section . If you are creating a new section or subsection, you should first modify the mkdocs.yml file to include it. Generally a section ( How to contribute? ) has several subsections ( MkDocs , Markdown ) and each of this subsections are pointing to its own file ( .md) in their corresponding /docs/ directory. Once you are done you will have to add your contributions to the version control using: git add <your file or directory> you can use the command git status to check the status of the files. Now that your new version is being tracked it is time to include your changes in the remote repo. git commit -m \"Add brief explanation of editions or contributions\" git push Once your commitments are pushed in the remote repository you can call MkDocs to re-built the web site including your contributions. This command is executed in the same directory as mkdocs.yml . To do so: mkdocs gh-deploy It is always a good idea to use mkdocs serve in order to see your contributions added to the web site locally before adding them officially to the online version. Congratulations you are now a contributor of our BioKT ToolBox!! :blush: Summary of commands for contribution work-flow: git pull origin master git add <file> git commit -m \"Explanation\" git push mkdocs serve mkdics gh-deploy","title":"Installation"},{"location":"MkDocs/installation.html#installing-mkdocs-and-other-software","text":"The files that are used by MkDocs to built this website are located in the BioKT/ResDocs repository on GitHub BioKT (let's call it remote repository). Although public, it requires some permissions to contribute directly. Once this permissions are aqcuired you are ready to start. In order to modify/create files, all the work will be done locally in a \"clone\" of the original remote repository. Changes made will be then \"pushed\" to the remote repo and used to re-built the website. The softwares required for this work are Git, MkDocs and presumably GitHub CLI. - Git is the version control to update/modify/create the files. - MkDocs builts the website. - GitHub CLI allows you to connect your local repo with the remote one via your GitHub credentials (HTTPS protocol).","title":"Installing MkDocs and other software."},{"location":"MkDocs/installation.html#installing-software","text":"","title":"Installing software."},{"location":"MkDocs/installation.html#git","text":"Windows: Git could be already installed. If not, go to the Git website and download it. MacOS: Check if Git is already available: git --version . If not, it will prompt you to install it. Linux: Ubuntu: sudo apt install git-all should work.","title":"Git."},{"location":"MkDocs/installation.html#mkdocs","text":"MkDocs requires a recent version of Python and the python package manager pip. MacOS: Simply run the following command: pip install mkdocs . Linux: Simply run the command : pip install mkdocs . Windows: Run: 'py -m pip install mkdocs'","title":"MkDocs."},{"location":"MkDocs/installation.html#github-cli","text":"MacOS: Via Conda with the command: conda install gh --channel conda-forge . Once installed, in the command line enter gh auth login and follow the instructions. Select HTTPS as preferred protocol. Use GitHub credentials to authenticate to Git. Linux: Follow the instructions here","title":"GitHub CLI."},{"location":"MkDocs/installation.html#contribute-process","text":"Now you are ready to start your contributions! First thing you want to do is create a directory in your computer to store the clone of the remote repository: mkdir BioKT_toolbox cd BioKT_toolbox Inside this directory clone the remote repository with: git clone https://github.com/BioKT/ResDocs.git If this is your first time, the cloned repository will be up to date. However, whenever in the future you want to contribute, you will need to ensure that your local repository is updated with the last contributions. Otherwise some unexpected errors will occur when trying to push your contributions into the remote repo. In order to update your local repo it would be enough to run the command below. REMEMBER, YOU SHOULD RUN THIS COMMAND EVERY SINGLE TIME YOU START A NEW CONTRIBUTION . git pull origin master If this is your first time updating the local repo, you will probably get a message talking about how should Git behave when it has to reconcile changes from your local repo and the remote repo. Just copy the first line that it recommends, something like: git config pull.rebase false # merge (the default strategy) Now you can modify the corresponding files of any section you want or create your new section or subsections. Modifying already existing files is easy. Just use \"vi\" or whatever text editor you like and then save the changes. If you want to know how the syntax works (which is Markdown btw), check this section . If you are creating a new section or subsection, you should first modify the mkdocs.yml file to include it. Generally a section ( How to contribute? ) has several subsections ( MkDocs , Markdown ) and each of this subsections are pointing to its own file ( .md) in their corresponding /docs/ directory. Once you are done you will have to add your contributions to the version control using: git add <your file or directory> you can use the command git status to check the status of the files. Now that your new version is being tracked it is time to include your changes in the remote repo. git commit -m \"Add brief explanation of editions or contributions\" git push Once your commitments are pushed in the remote repository you can call MkDocs to re-built the web site including your contributions. This command is executed in the same directory as mkdocs.yml . To do so: mkdocs gh-deploy It is always a good idea to use mkdocs serve in order to see your contributions added to the web site locally before adding them officially to the online version. Congratulations you are now a contributor of our BioKT ToolBox!! :blush: Summary of commands for contribution work-flow: git pull origin master git add <file> git commit -m \"Explanation\" git push mkdocs serve mkdics gh-deploy","title":"Contribute process."},{"location":"Other/other.html","text":"Advice on writing scientific papers The Art of Writing Science, by Kevin W. Plaxco ( Protein Sci., 2010 ). Whitesides' Group: Writing a Paper, by G. M. Whitesides ( Adv. Materials, 2004 ).","title":"Other"},{"location":"Other/other.html#advice-on-writing-scientific-papers","text":"The Art of Writing Science, by Kevin W. Plaxco ( Protein Sci., 2010 ). Whitesides' Group: Writing a Paper, by G. M. Whitesides ( Adv. Materials, 2004 ).","title":"Advice on writing scientific papers"},{"location":"PBS/documentation.html","text":"Installing Torque Bellow follow instructions on how to install Torque in a multiprocessor Ubuntu Linux server. In this case the same machine is used as server, scheduler, submission node and compute node. These notes have been borrowed from this blog post (thanks!) and are kept here for future records only. The version of Ubuntu used in this case was 14.04 LTS. The first thing to note is that you should do all of these as root . Then we must ensure that the first line in the /etc/hosts file reads as follows 127.0.0.1 localhost Next comes the installation of some packages, which we do using Ubuntu`s package manager. apt-get install torque-server torque-client torque-mom torque-pam After this step we simply stop these services since, apparently, the initial torque configuration does not really work as one would hope. In order to achieve this we simply type the following in the terminal /etc/init.d/torque-mom stop /etc/init.d/torque-scheduler stop /etc/init.d/torque-server stop We then can create a new setup for torque using the following pbs_server -t create When prompted about whether we want to overwrite the existing database we will reply yes ( [y] ). Next the just-started server instance is killed using the following command for further configuration killall pbs_server Next we will set up the server process. In my case the server is simply called localhost and I experienced some problems when trying to use a different server domain. echo localhost > /etc/torque/server_name echo localhost > /var/spool/torque/server_priv/acl_svr/acl_hosts echo root@localhost > /var/spool/torque/server_priv/acl_svr/operators echo root@localhost > /var/spool/torque/server_priv/acl_svr/managers The following step is to simply add the compute nodes. Since here we are using the \"head node\" as \"compute node\" then we just need to type the following echo \"localhost np=56\" > /var/spool/torque/server_priv/nodes Then we start the MOM process that handles the compute node echo localhost > /var/spool/torque/mom_priv/config After all of these one has to restart the processes again /etc/init.d/torque-server start /etc/init.d/torque-scheduler start /etc/init.d/torque-mom start Finally we need to restart the scheduler, create the default queue and configure thee server to allow submissions from itself qmgr -c \"set server scheduling = true\" qmgr -c \"set server keep_completed = 300\" qmgr -c \"set server mom_job_sync = true\" # create default queue qmgr -c \"create queue batch\" qmgr -c \"set queue batch queue_type = execution\" qmgr -c \"set queue batch started = true\" qmgr -c \"set queue batch enabled = true\" qmgr -c \"set queue batch resources_default.walltime = 1:00:00\" qmgr -c \"set queue batch resources_default.nodes = 1\" qmgr -c \"set server default_queue = batch\" # configure submission pool qmgr -c \"set server submit_hosts = localhost\" qmgr -c \"set server allow_node_submit = true\" Finally you can test whether everything is working right for you using the following command qsub -I An additional test script that can be done is to run this simples PBS script test.sh #!/bin/bash cd $PBS_O_WORKDIR #direct the output to cluster_nodes cat $PBS_NODEFILE > ./cluster_nodes This should run by simply writing the following command on your terminal qsub test.sh Example PBS script This is just an example PBS script for submitting jobs in the Archer supercomputing facility. #!/bin/bash --login #PBS -N jobname # Select 1 node #PBS -l select=1 #PBS -l walltime=24:00:00 #PBS -m abe #PBS -M name@emailprovider.org # Replace this with your budget code #PBS -A budget # Move to directory that script was submitted from #export PBS_O_WORKDIR=$(readlink -f $PBS_O_WORKDIR) #echo $PBS_O_WORKDIR #exit #cd $PBS_O_WORKDIR cd \"/work/directory/\" # Load the GROMACS module module add gromacs # Run GROMACS using default input and output file names pull=1 k=100 options=\"-s ${file} -deffnm ${file}\" aprun -n 24 mdrun_mpi $options","title":"PBS scripts"},{"location":"PBS/documentation.html#installing-torque","text":"Bellow follow instructions on how to install Torque in a multiprocessor Ubuntu Linux server. In this case the same machine is used as server, scheduler, submission node and compute node. These notes have been borrowed from this blog post (thanks!) and are kept here for future records only. The version of Ubuntu used in this case was 14.04 LTS. The first thing to note is that you should do all of these as root . Then we must ensure that the first line in the /etc/hosts file reads as follows 127.0.0.1 localhost Next comes the installation of some packages, which we do using Ubuntu`s package manager. apt-get install torque-server torque-client torque-mom torque-pam After this step we simply stop these services since, apparently, the initial torque configuration does not really work as one would hope. In order to achieve this we simply type the following in the terminal /etc/init.d/torque-mom stop /etc/init.d/torque-scheduler stop /etc/init.d/torque-server stop We then can create a new setup for torque using the following pbs_server -t create When prompted about whether we want to overwrite the existing database we will reply yes ( [y] ). Next the just-started server instance is killed using the following command for further configuration killall pbs_server Next we will set up the server process. In my case the server is simply called localhost and I experienced some problems when trying to use a different server domain. echo localhost > /etc/torque/server_name echo localhost > /var/spool/torque/server_priv/acl_svr/acl_hosts echo root@localhost > /var/spool/torque/server_priv/acl_svr/operators echo root@localhost > /var/spool/torque/server_priv/acl_svr/managers The following step is to simply add the compute nodes. Since here we are using the \"head node\" as \"compute node\" then we just need to type the following echo \"localhost np=56\" > /var/spool/torque/server_priv/nodes Then we start the MOM process that handles the compute node echo localhost > /var/spool/torque/mom_priv/config After all of these one has to restart the processes again /etc/init.d/torque-server start /etc/init.d/torque-scheduler start /etc/init.d/torque-mom start Finally we need to restart the scheduler, create the default queue and configure thee server to allow submissions from itself qmgr -c \"set server scheduling = true\" qmgr -c \"set server keep_completed = 300\" qmgr -c \"set server mom_job_sync = true\" # create default queue qmgr -c \"create queue batch\" qmgr -c \"set queue batch queue_type = execution\" qmgr -c \"set queue batch started = true\" qmgr -c \"set queue batch enabled = true\" qmgr -c \"set queue batch resources_default.walltime = 1:00:00\" qmgr -c \"set queue batch resources_default.nodes = 1\" qmgr -c \"set server default_queue = batch\" # configure submission pool qmgr -c \"set server submit_hosts = localhost\" qmgr -c \"set server allow_node_submit = true\" Finally you can test whether everything is working right for you using the following command qsub -I An additional test script that can be done is to run this simples PBS script test.sh #!/bin/bash cd $PBS_O_WORKDIR #direct the output to cluster_nodes cat $PBS_NODEFILE > ./cluster_nodes This should run by simply writing the following command on your terminal qsub test.sh","title":"Installing Torque"},{"location":"PBS/documentation.html#example-pbs-script","text":"This is just an example PBS script for submitting jobs in the Archer supercomputing facility. #!/bin/bash --login #PBS -N jobname # Select 1 node #PBS -l select=1 #PBS -l walltime=24:00:00 #PBS -m abe #PBS -M name@emailprovider.org # Replace this with your budget code #PBS -A budget # Move to directory that script was submitted from #export PBS_O_WORKDIR=$(readlink -f $PBS_O_WORKDIR) #echo $PBS_O_WORKDIR #exit #cd $PBS_O_WORKDIR cd \"/work/directory/\" # Load the GROMACS module module add gromacs # Run GROMACS using default input and output file names pull=1 k=100 options=\"-s ${file} -deffnm ${file}\" aprun -n 24 mdrun_mpi $options","title":"Example PBS script"},{"location":"Starting/guide.html","text":"Getting started guide There is so much to say about molecular dynamics (MD) simulations that at the beginning it may seem a bit daunting. Below we list a few resources that may be useful to step into the world of MD. Some useful books and reviews Frenkel & Smit, Understanding Molecular Simulation, From Algorithms to Applications (2nd. Ed., 2001). Allen & Tildesley, Computer Simulation of Liquids (2nd. Ed., 2017). See the companion repository . Best Practices for Foundations in Molecular Simulations, Braun et al. Living J. Comp. Mol. Sci. (2019). DOI: 10.33011/livecoms.1.1.5957 How to use the Gromacs software package Justin Lemkul's tutorials page: http://www.mdtutorials.com/ Analysis packages: The MDtraj Python library: http://mdtraj.org Coarse grained models SMOG : This webtool allows for you to generate parameters to run coarse-grained simulations in Gromacs with the famous Clementi model . Visualization VMD : An incredible program for visualizing your MD simulation data. You can find a tutorial here .","title":"Getting Started"},{"location":"Starting/guide.html#getting-started-guide","text":"There is so much to say about molecular dynamics (MD) simulations that at the beginning it may seem a bit daunting. Below we list a few resources that may be useful to step into the world of MD. Some useful books and reviews Frenkel & Smit, Understanding Molecular Simulation, From Algorithms to Applications (2nd. Ed., 2001). Allen & Tildesley, Computer Simulation of Liquids (2nd. Ed., 2017). See the companion repository . Best Practices for Foundations in Molecular Simulations, Braun et al. Living J. Comp. Mol. Sci. (2019). DOI: 10.33011/livecoms.1.1.5957 How to use the Gromacs software package Justin Lemkul's tutorials page: http://www.mdtutorials.com/ Analysis packages: The MDtraj Python library: http://mdtraj.org Coarse grained models SMOG : This webtool allows for you to generate parameters to run coarse-grained simulations in Gromacs with the famous Clementi model . Visualization VMD : An incredible program for visualizing your MD simulation data. You can find a tutorial here .","title":"Getting started guide"},{"location":"VMD/tools.html","text":"VMD scripting It is relatively easy to use the command line instead of the GUI in VMD. This is particularly useful when one needs to load multiple molecules, or make multiple visualizations look the same (e.g. making multiple molecules look like NewCartoon but changing their colour). Loading a molecule mol new file.pdb Applying a visualization style mol modstyle 0 0 NewCartoon Changing colour mol modcolor 0 0 ColorID 2 Loading a trajectory mol addfile traj_comp.xtc Loops in VMD The example below is for changing the colour of five different molecules so that each has a different colour. for {set x 0} {$x <= 5} {incr x} { mol modcolor 0 $x ColorID $x } Making videos in VMD VMD has a built in tool called Movie Maker. You must use it to generate a video according to your particular taste. You can find some instructions here . One possibility is that you save the files for each of the snapshots of the simulation, instead of letting VMD produce the movie and remove the files for you. If you do not do that, then you can tweak things a bit, in terms of using different movie formats or tuning the bitrate. Below is an example of the command that you can invoke for generating the video. ffmpeg -i filename.%05d.ppm -r 25 -an -b 10000k -bt 10000k moviename.mpg Clearly, you will need the ffmpeg program for doing this, which in a Mac you can obtain from MacPorts. Then the %0.5d in the filename just corresponds to the different files written by VMD that you want to process (be careful with format).","title":"VMD"},{"location":"VMD/tools.html#vmd-scripting","text":"It is relatively easy to use the command line instead of the GUI in VMD. This is particularly useful when one needs to load multiple molecules, or make multiple visualizations look the same (e.g. making multiple molecules look like NewCartoon but changing their colour).","title":"VMD scripting"},{"location":"VMD/tools.html#loading-a-molecule","text":"mol new file.pdb","title":"Loading a molecule"},{"location":"VMD/tools.html#applying-a-visualization-style","text":"mol modstyle 0 0 NewCartoon","title":"Applying a visualization style"},{"location":"VMD/tools.html#changing-colour","text":"mol modcolor 0 0 ColorID 2","title":"Changing colour"},{"location":"VMD/tools.html#loading-a-trajectory","text":"mol addfile traj_comp.xtc","title":"Loading a trajectory"},{"location":"VMD/tools.html#loops-in-vmd","text":"The example below is for changing the colour of five different molecules so that each has a different colour. for {set x 0} {$x <= 5} {incr x} { mol modcolor 0 $x ColorID $x }","title":"Loops in VMD"},{"location":"VMD/tools.html#making-videos-in-vmd","text":"VMD has a built in tool called Movie Maker. You must use it to generate a video according to your particular taste. You can find some instructions here . One possibility is that you save the files for each of the snapshots of the simulation, instead of letting VMD produce the movie and remove the files for you. If you do not do that, then you can tweak things a bit, in terms of using different movie formats or tuning the bitrate. Below is an example of the command that you can invoke for generating the video. ffmpeg -i filename.%05d.ppm -r 25 -an -b 10000k -bt 10000k moviename.mpg Clearly, you will need the ffmpeg program for doing this, which in a Mac you can obtain from MacPorts. Then the %0.5d in the filename just corresponds to the different files written by VMD that you want to process (be careful with format).","title":"Making videos in VMD"}]}